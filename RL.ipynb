{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Neural Networks: A Simple Tutorial \n",
    "\n",
    "By Dr. Fayyaz Minhas, DCIS, PIEAS (http://faculty.pieas.edu.pk/fayyaz/) (all rights reserved)\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This is a very simple tutorial for neural networks based Q-learning. We use a simple example of a magnetic levitation system. The learning objectives are listed below:\n",
    "\n",
    "1. Development of an understanding of the concepts of Q-learning\n",
    "2. How can neural networks be used for approximation of the Q-function\n",
    "3. How to develop a solution for a new problem using reinforcement learning\n",
    "4. How to develop a custom environment\n",
    "5. Complete end to end implementation of neural Q-learning in pyTorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement learning (RL) is a machine learning technique that lets an AI learn from its own experience. It has been around for quite a while now and has surfed the wave of neural networks and deep learning very well: See Alpha-Go, Alpha-Go Zero and Other DeepMind Projects. It also has the ability to lead to a general AI. \n",
    "\n",
    "To understand RL, it is interesting to understand the big picture first: Consider an agent that can interact with its environment by taking certain actions. The environment gives the agent a reward. As an example, consider a game of chess in which the player (agent) makes moves (an action) and receives a reward (win or lose). It is interesting to note that the reward can be delayed (till the end of the episode) and the agent may have no control over its environment which can change even without the actions of the player (e.g., by the action of the other player). Furthermore, the agent may have no knowledge of the internal working of the environment. The agent is to learn a \"policy\" that describes what actions to take in different states (board states in chess) so as to maximize its reward at the end of the episode (a single game). Think of reinforcement learning as a means of searching for the optimal policy. This is done using Q-learning.\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "Consider an agent in state $s_t$ at time $t$. Assume that the agent takes an action $a_t$ and moves to state $s_{t+1}$ where it gets a reward $r_t$ for its action leading to state $s_{t+1}$. The \"quality\" of an action, i.e., how good or bad an action $a$ is in a particular state $s$, can be described by a Q-function $Q(s,a)$. If we know the Q-function, we can determine what is the optimal action in a given state. Thus, Q-function models the policy of the agent. We want to learn a Q-function that  maximizes the reward for the agent. In order to learn the Q-function, assume that we initialize the Q-function randomly, i.e., it will generate a random score for a given (state,action) pair. We would like to update the current value of the Q-function $Q(s_t,a_t)$ based on the reward $r_t$ of moving to state $s_{t+1}$ from $s_t$ by taking action $a_t$: if the reward of being in state $s_t$ is high, the value of $Q(s_t,a_t)$ should be increased and vice-versa. Additionally, if the state $s_{t+1}$ is a \"favorable\" state, i.e., actions in state $s_{t+1}$ can lead to highly favorable situations then the value of $Q(s_t,a_t)$ should be increased and vice-versa. The favorability or goodness of the state $s_{t+1}$ (tecnically called utility $U(s_{t+1})$) can be described by the maximum Q-value of the best action in that state, i.e., $U(s_{t+1})=max_a Q(s_{t+1},a)$. This leads us to update the Q-function in the following manner: $Q(s_t,a_t)^{new} \\leftarrow (1-\\alpha)Q(s_t,a_t)+\\alpha(r_t+\\gamma max_a Q(s_{t+1},a)$. Here, $\\alpha \\in [0,1]$ and $\\gamma \\in (0,1)$, called learning rate and discount factor, respectively, are two constants. Note that the update is a weighted combination of the previous value of $Q$ and the information obtained as a consequence of the agent's action. Before we go into the detail of these constants, try to convince yourself that the update equation indeed follows the requirements for the update above, i.e., the Q-value is updated based on:\n",
    "\n",
    "1. Its previous value $Q(s_t,a_t)$: Higher the previous value, higher the new value\n",
    "2. Reward $r_t$: Higher the reward, higher the new value\n",
    "3. Utility of the state $U(s_{t+1})=max_a Q(s_{t+1},a)$: Higher the utility, higher the new  value\n",
    "\n",
    "### Role of learning rate\n",
    "\n",
    "The learning rate controls how much to add to the previous Q-value: If the old values are more reliable in comparison to the one obtained based on current reward and utility of the next state which can happen in noisy reward scenarios or uncertain utilities, the value of the learning rate should be set to low and high otherwise. However, if $\\alpha=0$, not learning takes place. In fully deterministic environments, a learning rate of 1.0 is optimal since we know that the information being received in response to an action does not contain any noise or uncertainty. When the problem is stochastic, the algorithm converges under some technical conditions on the learning rate that require it to decrease to zero. \n",
    "\n",
    "### Role of discount factor\n",
    "\n",
    "The discount factor controls the impact of the utility of the next state, i.e., it controls the effect of future rewards, since the utility value $U(s_{t+1})=max_a Q(s_{t+1},a)$ is based on Q-value of the state $s_{t+1}$ which is, in turn, based on rewards $r_{t+1}$ and $U(s_{t+2})$ and so on. To understand it better, note that the update equation for the Q-value is a recursive term and it involves $\\gamma U(s_{t+1})$ and $U(s_{t+1})$ will involve $\\gamma U(s_{t+2})$ and so on. Thus, the role of future utilies can be written as $\\gamma^k U(s_{t+k})$. As a consequence, the larger the value of $gamma$, the more terms of future utilities will be included in the update of the current Q-value. Setting $gamma$ close to zero, would make the agent greedy as it will consider only the current reward and may not learn an effective policy that will maximize the future rewards. This is analogous to a chess player who goes on a \"killing spree\" by killing opponent's pieces as and when possible without considering that allowing an opponent's piece to be on the board can lead to better states in the future.\n",
    "\n",
    "### Algorithm Description\n",
    "\n",
    "Now one might ask the question, how does this work in practice? This is how.\n",
    "\n",
    "One starts off with a random Q-value function. In a given episode, the agent is \"spawned\" in an intial state in which the agent makes a move based on the Q-value, i.e., pick an action based on the highest Q-value across all possible actions in that state. This will change the state of the agent and the agent will receive a reward. Based on this, the agent will update the Q-value of the previous state. Then the agent will pick another action and keep updating it until either the end or terminal state is reached or the user defined threshold on the number of steps in an episode expires. This is repeated for a number of episodes until the Q-values stop changing. It is important to note that if all the updates are based on optimal Q-values, the agent might not explore all possible states or actions. As a consequence, an $\\epsilon$-greedy policy is typically employed in whcih an agent takes random actions at a given state, instead of consulting the Q-value function, with a small probability $\\epsilon \\ge 0$. Typically, $\\epsilon$ is initialized to a high value (0.9) and gradually decreased over episodes or number of total steps. Similarly, the learning rate is also \"annealed\" or decreased gradually as the Q-function becomes more \"learned\" over episodes. The discount factor is also increased gradually to model the improved certainty of future rewards over episodes. \n",
    "\n",
    "### Implementation using Q-tables or Neural Networks\n",
    "\n",
    "It is interesting to note that $Q(s,a)$ is a function that, gives a value for a given state and action as input. We want to learn this function using the Q-learning update equation. If the states and actions are discrete and finite, then we can use a simple table that stores the Q-value of a given state-action pair. However, if the states or actions are continuous or infinite, this is not possible. For such cases, we can use neural networks.\n",
    "\n",
    "Neural networks are universal function approximators that can approximate the value of a function $f(x)$ based on its inputs $x$. This is done by minimizing an error or loss function between the output of a neural network $F(x)$ and the target value $f(x)$ over a set of training examples by updating the weight parameters of the network. \n",
    "\n",
    "We can use a neural network to \"store\" the Q-table or approximate the Q-function. Think of it this way: Instead of getting the Q-values from the Q-table, we will get them from the neural network. However, the issue with RL is that we do not know the optimal Q-function before hand and it must be learned on the fly. Thus, we will be simultaneously learning the Q-function and the neural network that approximates it. This is how it can be done.\n",
    "\n",
    "We can start off the agent in a certain state $s_t$ and use the neural network to give the Q-value for a given action by giving the neural network the current state-action pair or by using the $\\epsilon$ greedy policy that allows the agent to take random actions too. As a consequence, we will get a new Q value based on the update equation. We can store the current state-action pair and its updated Q-value in a memory or \"replay queue\" which can store up to a certain number of such training examples in it. We then do a certain number of training epochs over randomly selected input-output pairs where the input is the state-action pair and the output is the corresponding q-value. Repeating this process over a number of episodes will train the neural network to approximate the Q-function.\n",
    "\n",
    "However, we are going to start learning about it by controlling the position of a metallic ball using a magnet.\n",
    "\n",
    "## Magnetic Levitation System\n",
    "\n",
    "Consider a metallic ball of mass $m$ at positiopn $x_0$ moving with velocity $v_0$ at time $t$ which is acted upon by gravity in the downwards direction and a fixed force $F$ in the upward direction resulting from an electromagnet that can be switched on (action A = 1) or off (action A = 0) to control the position of the ball along the vertical. Thus, the net force $aF-mg$ causes acceleration $a=(AF-mg)/m$ in the mass. In a given time step $dt = 0.01$, the  velocity of the ball at time $t+dt$ becomoes $v = v_0+a*dt$ and its new position will be $x = x_0+v_dt+0.5a(dt)^2$. Thus far we have modeled the physics of the problem and the state of the system can be described by a vector $(v,x)$.\n",
    "\n",
    "The objective here is to develop an intelligent agent that can switch the electromagnet on or off so that the ball reaches a certain reference position. For this purpose, the environment gives the agent a reward in a given state based on how close the current position is to the target, i.e., $r=-|x-x_{ref}|$. \n",
    "\n",
    "We can implement this system as an Open AI Gym Environment that allows us to model the physics, visualize, and get rewards. The most important function is the step function which takes in an action and retursn the state and rewrard based on the system's current state. The complete implementation is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Aug 13 15:22:11 2018\n",
    "A simple example of a custom gym environment for a magnetic levitation system\n",
    "Imagine an iron ball of mass m placed at a certain location along the y-axis\n",
    "and a magentic force F pulling it up. How can we control its position?\n",
    "@author: Fayyaz Minhas, Noushan and Abdullah\n",
    "\"\"\"\n",
    "import gym\n",
    "from gym import spaces\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class MagLevEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}    \n",
    "    \n",
    "    GRAVITY = 9.8\n",
    "    FORCE = GRAVITY*2.0 # force is twice as string as gravity (2 Kg can be controlled)\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.__version__ = \"0.0.1.0\"\n",
    "        logging.info(\"MAGLevEnv - Version {}\".format(self.__version__))\n",
    "        \n",
    "        self.timestep = 0.01 #time step in every action\n",
    "        self.mass = 1.0\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(np.array([-20,-0.1]), np.array([+20, 10]), dtype=np.float32)\n",
    "        \n",
    "        self.lastAction = 0\n",
    "\n",
    "        self.referencepoint = 5.0\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action :\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs, reward, episode_over, info : tuple\n",
    "            ob (object) :\n",
    "                an environment-specific object representing your observation of\n",
    "                the environment.\n",
    "            reward (float) :\n",
    "                amount of reward achieved by the previous action. The scale\n",
    "                varies between environments, but the goal is always to increase\n",
    "                your total reward.\n",
    "            episode_over (bool) :\n",
    "                whether it's time to reset the environment again. Most (but not\n",
    "                all) tasks are divided up into well-defined episodes, and done\n",
    "                being True indicates the episode has terminated. (For example,\n",
    "                perhaps the pole tipped too far, or you lost your last life.)\n",
    "            info (dict) :\n",
    "                 diagnostic information useful for debugging. It can sometimes\n",
    "                 be useful for learning (for example, it might contain the raw\n",
    "                 probabilities behind the environment's last state change).\n",
    "                 However, official evaluations of your agent are not allowed to\n",
    "                 use this for learning.\n",
    "        \"\"\"\n",
    "        self._take_action(action)\n",
    "        self.lastAction = action\n",
    "        done = False\n",
    "        reward = self._get_reward()\n",
    "        obs = self._get_state()\n",
    "        \n",
    "        if not self.observation_space.contains(obs):\n",
    "            done = True\n",
    "        \n",
    "        return obs, reward, done, {}\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the state of the environment and returns an initial observation.\n",
    "        Returns\n",
    "        -------\n",
    "        observation (object): the initial observation of the space.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = random.randint(0,10)\n",
    "        v = (2*np.random.rand()-1)*20\n",
    "        \n",
    "        self.acceleration = 0\n",
    "        self.velocity = v\n",
    "        self.position = x\n",
    "        \n",
    "        \n",
    "        return self._get_state()\n",
    "\n",
    "    def render(self, figid = 0):\n",
    "        \"\"\"\n",
    "        Shows a ball with the position indicated by the position. Velocity is \n",
    "        proportional to the size of the ball. The current action is shown in \n",
    "        color of the ball with blue indicating no upward force and red if the\n",
    "        force is active.\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.figure(figid)\n",
    "        r = np.max((0.3,np.abs(self.velocity)/10.0))\n",
    "        c = 'b'\n",
    "        if self.lastAction:\n",
    "            c = 'r'\n",
    "        \n",
    "        \n",
    "        circle = plt.Circle((0,self.position), radius= r, color = c)\n",
    "        \n",
    "        ax=plt.gca()\n",
    "        ax.clear()\n",
    "        ax.add_patch(circle)\n",
    "        plt.axis('scaled')\n",
    "        plt.xlim(-10,10)\n",
    "        plt.ylim(-1,11)\n",
    "        plt.plot([-10,10],[self.referencepoint]*2)\n",
    "        plt.plot([-10,10],[0]*2)\n",
    "        plt.plot([-10,10],[10]*2)\n",
    "        plt.pause(0.00001)\n",
    "        plt.show()\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        \"\"\"\n",
    "        Model the effect of the action taken and update state.\n",
    "        Simple modeling of physics.\n",
    "        \"\"\"\n",
    "        \n",
    "        v0 = self.velocity\n",
    "        x0 = self.position   \n",
    "\n",
    "        a = ( ( action*MagLevEnv.FORCE / self.mass ) - MagLevEnv.GRAVITY )\n",
    "         \n",
    "        dv = ( a * self.timestep )\n",
    "        v = v0 + dv\n",
    "        dx = ( v0 * self.timestep ) + 0.5 * (a * self.timestep**2) \n",
    "        x = x0 + dx\n",
    "    \n",
    "        \n",
    "        self.acceleration = a\n",
    "        self.velocity = v\n",
    "        self.position = x\n",
    "        \n",
    "            \n",
    "    def _get_state(self):\n",
    "        \n",
    "        \"\"\"Get the observation.\"\"\"\n",
    "        \n",
    "        obs = np.asarray(list((self.velocity,self.position)))\n",
    "        return obs\n",
    "            \n",
    "\n",
    "    def _get_reward(self):\n",
    "        \"\"\"\n",
    "        Reward function.\n",
    "        \"\"\"\n",
    "        state = self._get_state()\n",
    "        reward =  float(-np.abs(state[1]-self.referencepoint))#*float(np.abs(next_state[0])<0.1)\n",
    "        if np.abs(state[1]-self.referencepoint)<0.5:\n",
    "            reward+=2.0\n",
    "        if not self.observation_space.contains(state):\n",
    "            reward-=1.0            \n",
    "        return reward\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Mass: \"+str(self.mass)+\" Ref: \"+str(self.referencepoint)+\" State: \"+str(self._get_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can now create and environment and observe the effects of the actions as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mass: 1.0 Ref: 5.0 State: [15.78704357  2.        ]\n",
      "Mass: 1.0 Ref: 5.0 State: [15.88504357  2.15836044]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc7f9828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = MagLevEnv()\n",
    "print(env)\n",
    "env.step(1.0)\n",
    "print(env)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the complete implementation of a Neural Q-learning algorithm. A neural Q-learning program will consist of three parts: environment, neural network and Q-learning. We have already implemented the environemtn. We use a simple neural network with 2 inputs (corresponding to the two state variables), 200 hidden neurons with ReLU activations and 2 outputs (corresponding to Q-values for each action). For details, see the neural network class implementation below.  A single episode (executed by the run_episode function) consists of 500 steps in whcih the environment is initialized (reset) and the actions are determined using an epsilon greedy policy with annealed epsilon values together with the neural network (see the select_action function). At each step, the neural network picks up 64 examples at random and uses them to learn the Q-function (see the learn function). The number of episodes is set to 70. It is important to note that the only information of the environment visible to neural network is the one given by the environment (position, velocity and reward). The internal workings of the environment are not visible to Q-learning. We have a Q-learning rate of 1.0 and a discount factor of 0.98 since the environment is deterministric. The target position is 5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afsar\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Final Position Error 67.34598869061487 \n",
      "Episode 1 Final Position Error 7.474194257862854 \n",
      "Episode 2 Final Position Error 4.040735412302233 \n",
      "Episode 3 Final Position Error 4.256169825692844 \n",
      "Episode 4 Final Position Error 4.633465928788674 \n",
      "Episode 5 Final Position Error 48.651749126512215 \n",
      "Episode 6 Final Position Error 3.661552023046818 \n",
      "Episode 7 Final Position Error 12.971079417059403 \n",
      "Episode 8 Final Position Error 3.7136192037182125 \n",
      "Episode 9 Final Position Error 4.021670943240085 \n",
      "Episode 10 Final Position Error 3.5806922658259106 \n",
      "Episode 11 Final Position Error 5.867766206930719 \n",
      "Episode 12 Final Position Error 5.306329538974597 \n",
      "Episode 13 Final Position Error 0.06774287340356899 \n",
      "Episode 14 Final Position Error 7.380463384582615 \n",
      "Episode 15 Final Position Error 0.5049821180055227 \n",
      "Episode 16 Final Position Error 1.701994544479268 \n",
      "Episode 17 Final Position Error 2.842893899096688 \n",
      "Episode 18 Final Position Error 2.636614515728536 \n",
      "Episode 19 Final Position Error 0.2642030803743163 \n",
      "Episode 20 Final Position Error 1.7200698976095685 \n",
      "Episode 21 Final Position Error 0.9246766698255247 \n",
      "Episode 22 Final Position Error 0.0029741816929140796 \n",
      "Episode 23 Final Position Error 0.008234775566544172 \n",
      "Episode 24 Final Position Error 0.10513883362013399 \n",
      "Episode 25 Final Position Error 0.00149624945075022 \n",
      "Episode 26 Final Position Error 0.12560920988018065 \n",
      "Episode 27 Final Position Error 0.006191449703305096 \n",
      "Episode 28 Final Position Error 0.03718210534760136 \n",
      "Episode 29 Final Position Error 0.0820150547460301 \n",
      "Episode 30 Final Position Error 0.0012517445779876013 \n",
      "Episode 31 Final Position Error 3.6760092636064847 \n",
      "Episode 32 Final Position Error 0.07441749350389859 \n",
      "Episode 33 Final Position Error 0.06057288891009982 \n",
      "Episode 34 Final Position Error 39.98037944843179 \n",
      "Episode 35 Final Position Error 0.06721423545195204 \n",
      "Episode 36 Final Position Error 0.046908893633621496 \n",
      "Episode 37 Final Position Error 0.007912568733243752 \n",
      "Episode 38 Final Position Error 0.006472908005933142 \n",
      "Episode 39 Final Position Error 0.04001815666520514 \n",
      "Episode 40 Final Position Error 3.80289413201874 \n",
      "Episode 41 Final Position Error 16.21449326924236 \n",
      "Episode 42 Final Position Error 0.2869680335538778 \n",
      "Episode 43 Final Position Error 0.03648668412472578 \n",
      "Episode 44 Final Position Error 0.6838081997606906 \n",
      "Episode 45 Final Position Error 0.049067625877322385 \n",
      "Episode 46 Final Position Error 0.0015894435364289805 \n",
      "Episode 47 Final Position Error 2.4974596025120315 \n",
      "Episode 48 Final Position Error 0.028212475587966246 \n",
      "Episode 49 Final Position Error 0.27477146990234047 \n",
      "Episode 50 Final Position Error 0.03696827227778865 \n",
      "Episode 51 Final Position Error 0.02453598546115554 \n",
      "Episode 52 Final Position Error 3.665541304333628 \n",
      "Episode 53 Final Position Error 0.004348087556619085 \n",
      "Episode 54 Final Position Error 0.002471767784033574 \n",
      "Episode 55 Final Position Error 0.05133434578680074 \n",
      "Episode 56 Final Position Error 0.04234453219606138 \n",
      "Episode 57 Final Position Error 0.03215544494600753 \n",
      "Episode 58 Final Position Error 0.03262205269487506 \n",
      "Episode 59 Final Position Error 0.08746297893447519 \n",
      "Episode 60 Final Position Error 0.02324067439638533 \n",
      "Episode 61 Final Position Error 1.3115380118463058 \n",
      "Episode 62 Final Position Error 0.1368416921021458 \n",
      "Episode 63 Final Position Error 0.0417406819006132 \n",
      "Episode 64 Final Position Error 6.915648550617888 \n",
      "Episode 65 Final Position Error 2.430631970611404 \n",
      "Episode 66 Final Position Error 1.5340664062554175 \n",
      "Episode 67 Final Position Error 0.06663555508501329 \n",
      "Episode 68 Final Position Error 0.03519997388574314 \n",
      "Episode 69 Final Position Error 1.8292139296973304 \n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Mon Aug 13 15:22:11 2018\n",
    "A simple example of a magnetic levitation system controlled using Reinforcement Learning\n",
    "@author: Fayyaz Minhas\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "#from maglevEnv import MagLevEnv\n",
    "import numpy as np\n",
    "\n",
    "# hyper parameters\n",
    "EPISODES = 70  # number of episodes\n",
    "EPS_START = 0.9  # e-greedy threshold start value\n",
    "EPS_END = 0.01 # e-greedy threshold end value\n",
    "EPS_DECAY = 1000  # e-greedy threshold decay\n",
    "GAMMA = 0.98  # Q-learning discount factor\n",
    "LR = 0.005  # NN optimizer learning rate\n",
    "BATCH_SIZE = 64  # Q-learning batch size\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(2, 200)\n",
    "        self.l3 = nn.Linear(200, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = (self.l3(x))\n",
    "        return x\n",
    "\n",
    "env = MagLevEnv()\n",
    "env.referencepoint = 5.0\n",
    "\n",
    "model = Network()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "memory = ReplayMemory(10000)\n",
    "optimizer = optim.Adam(model.parameters(), LR)\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]])\n",
    "\n",
    "\n",
    "def run_episode(e, environment):\n",
    "    ref = environment.referencepoint\n",
    "    state = environment.reset()    \n",
    "    steps = 0\n",
    "    while True:\n",
    "        steps += 1\n",
    "        \n",
    "        action = select_action(FloatTensor([state]))\n",
    "        a = action.data.numpy()[0,0]\n",
    "        next_state, reward, done, _ = environment.step(a)\n",
    "\n",
    "\n",
    "\n",
    "        memory.push((FloatTensor([state]),\n",
    "                     action,  # action is already a tensor\n",
    "                     FloatTensor([next_state]),\n",
    "                     FloatTensor([reward])))\n",
    "\n",
    "        \n",
    "        learn()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if steps > 500:\n",
    "            print(\"Episode %s Final Position Error %s \" %(e, np.abs(next_state[1]-ref)))\n",
    "            episode_durations.append(np.abs(next_state[1]-ref))\n",
    "            #plotError()\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "def learn():\n",
    "    global GAMMA\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "\n",
    "    batch_state = Variable(torch.cat(batch_state))\n",
    "    batch_action = Variable(torch.cat(batch_action))\n",
    "    batch_reward = Variable(torch.cat(batch_reward))\n",
    "    batch_next_state = Variable(torch.cat(batch_next_state))\n",
    "\n",
    "    # current Q values are estimated by NN for all actions\n",
    "    current_q_values = model(batch_state).gather(1, batch_action)\n",
    "    # expected Q values are estimated from actions which gives maximum Q value\n",
    "    max_next_q_values = model(batch_next_state).detach().max(1)[0]\n",
    "        \n",
    "\n",
    "    expected_q_values = batch_reward + (GAMMA * max_next_q_values)\n",
    "\n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(current_q_values, expected_q_values.view(BATCH_SIZE,1))\n",
    "\n",
    "    # backpropagation of loss to NN\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "def plotError():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title(str(steps_done))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Final Position Error')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # take 100 episode averages and plot them too\n",
    "    H = 10\n",
    "    if len(durations_t) >= H:\n",
    "        means = durations_t.unfold(0, H, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(H-1), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    run_episode(e, env)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the learning is complete, we can test the performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.902    2.99049] 1 -2.00951 False\n",
      "1 [-0.804    2.98196] 1 -2.01804 False\n",
      "2 [-0.706    2.97441] 1 -2.0255900000000002 False\n",
      "3 [-0.608    2.96784] 1 -2.03216 False\n",
      "4 [-0.51     2.96225] 1 -2.0377500000000004 False\n",
      "5 [-0.412    2.95764] 1 -2.0423600000000004 False\n",
      "6 [-0.314    2.95401] 1 -2.04599 False\n",
      "7 [-0.216    2.95136] 1 -2.0486400000000002 False\n",
      "8 [-0.118    2.94969] 1 -2.05031 False\n",
      "9 [-0.02   2.949] 1 -2.051 False\n",
      "10 [0.078   2.94929] 1 -2.05071 False\n",
      "11 [0.176   2.95056] 1 -2.04944 False\n",
      "12 [0.274   2.95281] 1 -2.04719 False\n",
      "13 [0.372   2.95604] 1 -2.04396 False\n",
      "14 [0.47    2.96025] 1 -2.03975 False\n",
      "15 [0.568   2.96544] 1 -2.0345600000000004 False\n",
      "16 [0.666   2.97161] 1 -2.0283900000000004 False\n",
      "17 [0.764   2.97876] 1 -2.02124 False\n",
      "18 [0.862   2.98689] 1 -2.01311 False\n",
      "19 [0.96  2.996] 1 -2.004 False\n",
      "20 [1.058   3.00609] 1 -1.99391 False\n",
      "21 [1.156   3.01716] 1 -1.98284 False\n",
      "22 [1.254   3.02921] 1 -1.97079 False\n",
      "23 [1.352   3.04224] 1 -1.95776 False\n",
      "24 [1.45    3.05625] 1 -1.94375 False\n",
      "25 [1.548   3.07124] 1 -1.92876 False\n",
      "26 [1.646   3.08721] 1 -1.9127900000000002 False\n",
      "27 [1.744   3.10416] 1 -1.8958400000000002 False\n",
      "28 [1.842   3.12209] 1 -1.87791 False\n",
      "29 [1.94  3.141] 1 -1.859 False\n",
      "30 [2.038   3.16089] 1 -1.8391099999999998 False\n",
      "31 [2.136   3.18176] 1 -1.8182399999999999 False\n",
      "32 [2.234   3.20361] 1 -1.7963899999999997 False\n",
      "33 [2.332   3.22644] 1 -1.7735599999999998 False\n",
      "34 [2.234   3.24927] 0 -1.75073 False\n",
      "35 [2.136   3.27112] 0 -1.7288799999999998 False\n",
      "36 [2.234   3.29297] 1 -1.7070299999999996 False\n",
      "37 [2.136   3.31482] 0 -1.6851799999999995 False\n",
      "38 [2.038   3.33569] 0 -1.6643099999999995 False\n",
      "39 [2.136   3.35656] 1 -1.6434399999999996 False\n",
      "40 [2.038   3.37743] 0 -1.6225699999999996 False\n",
      "41 [1.94    3.39732] 0 -1.6026799999999994 False\n",
      "42 [2.038   3.41721] 1 -1.5827899999999993 False\n",
      "43 [1.94   3.4371] 0 -1.562899999999999 False\n",
      "44 [1.842   3.45601] 0 -1.543989999999999 False\n",
      "45 [1.94    3.47492] 1 -1.525079999999999 False\n",
      "46 [1.842   3.49383] 0 -1.5061699999999991 False\n",
      "47 [1.94    3.51274] 1 -1.4872599999999991 False\n",
      "48 [1.842   3.53165] 0 -1.4683499999999992 False\n",
      "49 [1.744   3.54958] 0 -1.450419999999999 False\n",
      "50 [1.842   3.56751] 1 -1.4324899999999987 False\n",
      "51 [1.744   3.58544] 0 -1.4145599999999985 False\n",
      "52 [1.646   3.60239] 0 -1.3976099999999985 False\n",
      "53 [1.744   3.61934] 1 -1.3806599999999984 False\n",
      "54 [1.646   3.63629] 0 -1.3637099999999984 False\n",
      "55 [1.744   3.65324] 1 -1.3467599999999984 False\n",
      "56 [1.646   3.67019] 0 -1.3298099999999984 False\n",
      "57 [1.548   3.68616] 0 -1.3138399999999986 False\n",
      "58 [1.646   3.70213] 1 -1.2978699999999987 False\n",
      "59 [1.548  3.7181] 0 -1.281899999999999 False\n",
      "60 [1.646   3.73407] 1 -1.2659299999999991 False\n",
      "61 [1.548   3.75004] 0 -1.2499599999999993 False\n",
      "62 [1.45    3.76503] 0 -1.2349699999999992 False\n",
      "63 [1.548   3.78002] 1 -1.2199799999999992 False\n",
      "64 [1.45    3.79501] 0 -1.2049899999999991 False\n",
      "65 [1.548 3.81 ] 1 -1.189999999999999 False\n",
      "66 [1.45    3.82499] 0 -1.175009999999999 False\n",
      "67 [1.352 3.839] 0 -1.1609999999999991 False\n",
      "68 [1.45    3.85301] 1 -1.1469899999999993 False\n",
      "69 [1.352   3.86702] 0 -1.1329799999999994 False\n",
      "70 [1.45    3.88103] 1 -1.1189699999999996 False\n",
      "71 [1.352   3.89504] 0 -1.1049599999999997 False\n",
      "72 [1.254   3.90807] 0 -1.0919299999999996 False\n",
      "73 [1.352  3.9211] 1 -1.0788999999999995 False\n",
      "74 [1.254   3.93413] 0 -1.0658699999999994 False\n",
      "75 [1.156   3.94618] 0 -1.0538199999999995 False\n",
      "76 [1.254   3.95823] 1 -1.0417699999999996 False\n",
      "77 [1.156   3.97028] 0 -1.0297199999999997 False\n",
      "78 [1.254   3.98233] 1 -1.0176699999999999 False\n",
      "79 [1.156   3.99438] 0 -1.00562 False\n",
      "80 [1.254   4.00643] 1 -0.9935700000000001 False\n",
      "81 [1.156   4.01848] 0 -0.9815199999999997 False\n",
      "82 [1.058   4.02955] 0 -0.9704499999999996 False\n",
      "83 [1.156   4.04062] 1 -0.9593799999999995 False\n",
      "84 [1.058   4.05169] 0 -0.9483099999999993 False\n",
      "85 [1.156   4.06276] 1 -0.9372399999999992 False\n",
      "86 [1.058   4.07383] 0 -0.926169999999999 False\n",
      "87 [1.156  4.0849] 1 -0.9150999999999989 False\n",
      "88 [1.058   4.09597] 0 -0.9040299999999988 False\n",
      "89 [1.156   4.10704] 1 -0.8929599999999986 False\n",
      "90 [1.058   4.11811] 0 -0.8818899999999985 False\n",
      "91 [0.96   4.1282] 0 -0.8717999999999986 False\n",
      "92 [1.058   4.13829] 1 -0.8617099999999986 False\n",
      "93 [0.96    4.14838] 0 -0.8516199999999987 False\n",
      "94 [1.058   4.15847] 1 -0.8415299999999988 False\n",
      "95 [0.96    4.16856] 0 -0.8314399999999988 False\n",
      "96 [1.058   4.17865] 1 -0.8213499999999989 False\n",
      "97 [0.96    4.18874] 0 -0.811259999999999 False\n",
      "98 [0.862   4.19785] 0 -0.8021499999999993 False\n",
      "99 [0.96    4.20696] 1 -0.7930399999999995 False\n",
      "100 [0.862   4.21607] 0 -0.7839299999999998 False\n",
      "101 [0.96    4.22518] 1 -0.7748200000000001 False\n",
      "102 [0.862   4.23429] 0 -0.7657100000000003 False\n",
      "103 [0.96   4.2434] 1 -0.7566000000000006 False\n",
      "104 [0.862   4.25251] 0 -0.7474900000000009 False\n",
      "105 [0.96    4.26162] 1 -0.7383800000000011 False\n",
      "106 [0.862   4.27073] 0 -0.7292700000000014 False\n",
      "107 [0.764   4.27886] 0 -0.721140000000001 False\n",
      "108 [0.862   4.28699] 1 -0.7130100000000006 False\n",
      "109 [0.764   4.29512] 0 -0.7048800000000002 False\n",
      "110 [0.862   4.30325] 1 -0.6967499999999998 False\n",
      "111 [0.764   4.31138] 0 -0.6886199999999993 False\n",
      "112 [0.862   4.31951] 1 -0.6804899999999989 False\n",
      "113 [0.764   4.32764] 0 -0.6723599999999985 False\n",
      "114 [0.862   4.33577] 1 -0.6642299999999981 False\n",
      "115 [0.764  4.3439] 0 -0.6560999999999977 False\n",
      "116 [0.862   4.35203] 1 -0.6479699999999973 False\n",
      "117 [0.764   4.36016] 0 -0.6398399999999969 False\n",
      "118 [0.666   4.36731] 0 -0.6326899999999966 False\n",
      "119 [0.764   4.37446] 1 -0.6255399999999964 False\n",
      "120 [0.666   4.38161] 0 -0.6183899999999962 False\n",
      "121 [0.764   4.38876] 1 -0.611239999999996 False\n",
      "122 [0.666   4.39591] 0 -0.6040899999999958 False\n",
      "123 [0.764   4.40306] 1 -0.5969399999999956 False\n",
      "124 [0.666   4.41021] 0 -0.5897899999999954 False\n",
      "125 [0.764   4.41736] 1 -0.5826399999999952 False\n",
      "126 [0.666   4.42451] 0 -0.575489999999995 False\n",
      "127 [0.764   4.43166] 1 -0.5683399999999947 False\n",
      "128 [0.666   4.43881] 0 -0.5611899999999945 False\n",
      "129 [0.568   4.44498] 0 -0.5550199999999945 False\n",
      "130 [0.666   4.45115] 1 -0.5488499999999945 False\n",
      "131 [0.568   4.45732] 0 -0.5426799999999945 False\n",
      "132 [0.666   4.46349] 1 -0.5365099999999945 False\n",
      "133 [0.568   4.46966] 0 -0.5303399999999945 False\n",
      "134 [0.666   4.47583] 1 -0.5241699999999945 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afsar\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 [0.568 4.482] 0 -0.5179999999999945 False\n",
      "136 [0.666   4.48817] 1 -0.5118299999999945 False\n",
      "137 [0.568   4.49434] 0 -0.5056599999999944 False\n",
      "138 [0.666   4.50051] 1 1.5005100000000056 False\n",
      "139 [0.568   4.50668] 0 1.5066800000000056 False\n",
      "140 [0.666   4.51285] 1 1.5128500000000056 False\n",
      "141 [0.568   4.51902] 0 1.5190200000000056 False\n",
      "142 [0.47    4.52421] 0 1.5242100000000054 False\n",
      "143 [0.568  4.5294] 1 1.5294000000000052 False\n",
      "144 [0.47    4.53459] 0 1.534590000000005 False\n",
      "145 [0.568   4.53978] 1 1.5397800000000048 False\n",
      "146 [0.47    4.54497] 0 1.5449700000000046 False\n",
      "147 [0.568   4.55016] 1 1.5501600000000044 False\n",
      "148 [0.47    4.55535] 0 1.5553500000000042 False\n",
      "149 [0.568   4.56054] 1 1.560540000000004 False\n",
      "150 [0.47    4.56573] 0 1.5657300000000038 False\n",
      "151 [0.568   4.57092] 1 1.5709200000000036 False\n",
      "152 [0.47    4.57611] 0 1.5761100000000035 False\n",
      "153 [0.568  4.5813] 1 1.5813000000000033 False\n",
      "154 [0.47    4.58649] 0 1.586490000000003 False\n",
      "155 [0.568   4.59168] 1 1.5916800000000029 False\n",
      "156 [0.47    4.59687] 0 1.5968700000000027 False\n",
      "157 [0.372   4.60108] 0 1.6010800000000023 False\n",
      "158 [0.47    4.60529] 1 1.6052900000000019 False\n",
      "159 [0.372  4.6095] 0 1.6095000000000015 False\n",
      "160 [0.47    4.61371] 1 1.613710000000001 False\n",
      "161 [0.372   4.61792] 0 1.6179200000000007 False\n",
      "162 [0.47    4.62213] 1 1.6221300000000003 False\n",
      "163 [0.372   4.62634] 0 1.62634 False\n",
      "164 [0.47    4.63055] 1 1.6305499999999995 False\n",
      "165 [0.372   4.63476] 0 1.634759999999999 False\n",
      "166 [0.47    4.63897] 1 1.6389699999999987 False\n",
      "167 [0.372   4.64318] 0 1.6431799999999983 False\n",
      "168 [0.47    4.64739] 1 1.647389999999998 False\n",
      "169 [0.372  4.6516] 0 1.6515999999999975 False\n",
      "170 [0.274   4.65483] 0 1.6548299999999978 False\n",
      "171 [0.372   4.65806] 1 1.658059999999998 False\n",
      "172 [0.274   4.66129] 0 1.6612899999999984 False\n",
      "173 [0.372   4.66452] 1 1.6645199999999987 False\n",
      "174 [0.274   4.66775] 0 1.667749999999999 False\n",
      "175 [0.372   4.67098] 1 1.6709799999999992 False\n",
      "176 [0.274   4.67421] 0 1.6742099999999995 False\n",
      "177 [0.372   4.67744] 1 1.6774399999999998 False\n",
      "178 [0.274   4.68067] 0 1.68067 False\n",
      "179 [0.372  4.6839] 1 1.6839000000000004 False\n",
      "180 [0.274   4.68713] 0 1.6871300000000007 False\n",
      "181 [0.372   4.69036] 1 1.690360000000001 False\n",
      "182 [0.274   4.69359] 0 1.6935900000000013 False\n",
      "183 [0.372   4.69682] 1 1.6968200000000015 False\n",
      "184 [0.274   4.70005] 0 1.7000500000000018 False\n",
      "185 [0.372   4.70328] 1 1.7032800000000021 False\n",
      "186 [0.274   4.70651] 0 1.7065100000000024 False\n",
      "187 [0.372   4.70974] 1 1.7097400000000027 False\n",
      "188 [0.274   4.71297] 0 1.712970000000003 False\n",
      "189 [0.176   4.71522] 0 1.715220000000003 False\n",
      "190 [0.274   4.71747] 1 1.7174700000000032 False\n",
      "191 [0.176   4.71972] 0 1.7197200000000032 False\n",
      "192 [0.274   4.72197] 1 1.7219700000000033 False\n",
      "193 [0.176   4.72422] 0 1.7242200000000034 False\n",
      "194 [0.274   4.72647] 1 1.7264700000000035 False\n",
      "195 [0.176   4.72872] 0 1.7287200000000036 False\n",
      "196 [0.274   4.73097] 1 1.7309700000000037 False\n",
      "197 [0.176   4.73322] 0 1.7332200000000038 False\n",
      "198 [0.274   4.73547] 1 1.7354700000000038 False\n",
      "199 [0.176   4.73772] 0 1.737720000000004 False\n",
      "200 [0.274   4.73997] 1 1.739970000000004 False\n",
      "201 [0.176   4.74222] 0 1.742220000000004 False\n",
      "202 [0.274   4.74447] 1 1.7444700000000042 False\n",
      "203 [0.176   4.74672] 0 1.7467200000000043 False\n",
      "204 [0.274   4.74897] 1 1.7489700000000044 False\n",
      "205 [0.176   4.75122] 0 1.7512200000000044 False\n",
      "206 [0.274   4.75347] 1 1.7534700000000045 False\n",
      "207 [0.176   4.75572] 0 1.7557200000000046 False\n",
      "208 [0.274   4.75797] 1 1.7579700000000047 False\n",
      "209 [0.176   4.76022] 0 1.7602200000000048 False\n",
      "210 [0.274   4.76247] 1 1.7624700000000049 False\n",
      "211 [0.176   4.76472] 0 1.764720000000005 False\n",
      "212 [0.078   4.76599] 0 1.7659900000000048 False\n",
      "213 [0.176   4.76726] 1 1.7672600000000047 False\n",
      "214 [0.078   4.76853] 0 1.7685300000000046 False\n",
      "215 [0.176  4.7698] 1 1.7698000000000045 False\n",
      "216 [0.078   4.77107] 0 1.7710700000000044 False\n",
      "217 [0.176   4.77234] 1 1.7723400000000042 False\n",
      "218 [0.078   4.77361] 0 1.7736100000000041 False\n",
      "219 [0.176   4.77488] 1 1.774880000000004 False\n",
      "220 [0.078   4.77615] 0 1.776150000000004 False\n",
      "221 [0.176   4.77742] 1 1.7774200000000038 False\n",
      "222 [0.078   4.77869] 0 1.7786900000000037 False\n",
      "223 [0.176   4.77996] 1 1.7799600000000035 False\n",
      "224 [0.078   4.78123] 0 1.7812300000000034 False\n",
      "225 [0.176  4.7825] 1 1.7825000000000033 False\n",
      "226 [0.078   4.78377] 0 1.7837700000000032 False\n",
      "227 [0.176   4.78504] 1 1.785040000000003 False\n",
      "228 [0.078   4.78631] 0 1.786310000000003 False\n",
      "229 [0.176   4.78758] 1 1.7875800000000028 False\n",
      "230 [0.078   4.78885] 0 1.7888500000000027 False\n",
      "231 [0.176   4.79012] 1 1.7901200000000026 False\n",
      "232 [0.078   4.79139] 0 1.7913900000000025 False\n",
      "233 [0.176   4.79266] 1 1.7926600000000024 False\n",
      "234 [0.078   4.79393] 0 1.7939300000000022 False\n",
      "235 [0.176  4.7952] 1 1.7952000000000021 False\n",
      "236 [0.078   4.79647] 0 1.796470000000002 False\n",
      "237 [0.176   4.79774] 1 1.797740000000002 False\n",
      "238 [0.078   4.79901] 0 1.7990100000000018 False\n",
      "239 [0.176   4.80028] 1 1.8002800000000017 False\n",
      "240 [0.078   4.80155] 0 1.8015500000000015 False\n",
      "241 [0.176   4.80282] 1 1.8028200000000014 False\n",
      "242 [0.078   4.80409] 0 1.8040900000000013 False\n",
      "243 [0.176   4.80536] 1 1.8053600000000012 False\n",
      "244 [0.078   4.80663] 0 1.806630000000001 False\n",
      "245 [0.176  4.8079] 1 1.807900000000001 False\n",
      "246 [0.078   4.80917] 0 1.8091700000000008 False\n",
      "247 [0.176   4.81044] 1 1.8104400000000007 False\n",
      "248 [0.078   4.81171] 0 1.8117100000000006 False\n",
      "249 [0.176   4.81298] 1 1.8129800000000005 False\n",
      "250 [0.078   4.81425] 0 1.8142500000000004 False\n",
      "251 [0.176   4.81552] 1 1.8155200000000002 False\n",
      "252 [0.078   4.81679] 0 1.8167900000000001 False\n",
      "253 [-0.02     4.81708] 0 1.8170799999999998 False\n",
      "254 [0.078   4.81737] 1 1.8173699999999995 False\n",
      "255 [-0.02     4.81766] 0 1.8176599999999992 False\n",
      "256 [0.078   4.81795] 1 1.8179499999999988 False\n",
      "257 [-0.02     4.81824] 0 1.8182399999999985 False\n",
      "258 [0.078   4.81853] 1 1.8185299999999982 False\n",
      "259 [-0.02     4.81882] 0 1.8188199999999979 False\n",
      "260 [0.078   4.81911] 1 1.8191099999999976 False\n",
      "261 [-0.02    4.8194] 0 1.8193999999999972 False\n",
      "262 [0.078   4.81969] 1 1.819689999999997 False\n",
      "263 [-0.02     4.81998] 0 1.8199799999999966 False\n",
      "264 [0.078   4.82027] 1 1.8202699999999963 False\n",
      "265 [-0.02     4.82056] 0 1.820559999999996 False\n",
      "266 [0.078   4.82085] 1 1.8208499999999956 False\n",
      "267 [-0.02     4.82114] 0 1.8211399999999953 False\n",
      "268 [0.078   4.82143] 1 1.821429999999995 False\n",
      "269 [-0.02     4.82172] 0 1.8217199999999947 False\n",
      "270 [0.078   4.82201] 1 1.8220099999999944 False\n",
      "271 [-0.02    4.8223] 0 1.822299999999994 False\n",
      "272 [0.078   4.82259] 1 1.8225899999999937 False\n",
      "273 [-0.02     4.82288] 0 1.8228799999999934 False\n",
      "274 [0.078   4.82317] 1 1.823169999999993 False\n",
      "275 [-0.02     4.82346] 0 1.8234599999999928 False\n",
      "276 [0.078   4.82375] 1 1.8237499999999924 False\n",
      "277 [-0.02     4.82404] 0 1.8240399999999921 False\n",
      "278 [0.078   4.82433] 1 1.8243299999999918 False\n",
      "279 [-0.02     4.82462] 0 1.8246199999999915 False\n",
      "280 [0.078   4.82491] 1 1.8249099999999912 False\n",
      "281 [-0.02    4.8252] 0 1.8251999999999908 False\n",
      "282 [0.078   4.82549] 1 1.8254899999999905 False\n",
      "283 [-0.02     4.82578] 0 1.8257799999999902 False\n",
      "284 [0.078   4.82607] 1 1.8260699999999899 False\n",
      "285 [0.176   4.82734] 1 1.8273399999999898 False\n",
      "286 [0.078   4.82861] 0 1.8286099999999896 False\n",
      "287 [-0.02    4.8289] 0 1.8288999999999893 False\n",
      "288 [0.078   4.82919] 1 1.829189999999989 False\n",
      "289 [-0.02     4.82948] 0 1.8294799999999887 False\n",
      "290 [0.078   4.82977] 1 1.8297699999999884 False\n",
      "291 [-0.02     4.83006] 0 1.830059999999988 False\n",
      "292 [0.078   4.83035] 1 1.8303499999999877 False\n",
      "293 [-0.02     4.83064] 0 1.8306399999999874 False\n",
      "294 [0.078   4.83093] 1 1.830929999999987 False\n",
      "295 [-0.02     4.83122] 0 1.8312199999999867 False\n",
      "296 [0.078   4.83151] 1 1.8315099999999864 False\n",
      "297 [-0.02    4.8318] 0 1.831799999999986 False\n",
      "298 [0.078   4.83209] 1 1.8320899999999858 False\n",
      "299 [-0.02     4.83238] 0 1.8323799999999855 False\n",
      "300 [0.078   4.83267] 1 1.8326699999999851 False\n",
      "301 [-0.02     4.83296] 0 1.8329599999999848 False\n",
      "302 [0.078   4.83325] 1 1.8332499999999845 False\n",
      "303 [-0.02     4.83354] 0 1.8335399999999842 False\n",
      "304 [0.078   4.83383] 1 1.8338299999999839 False\n",
      "305 [-0.02     4.83412] 0 1.8341199999999835 False\n",
      "306 [0.078   4.83441] 1 1.8344099999999832 False\n",
      "307 [-0.02    4.8347] 0 1.834699999999983 False\n",
      "308 [0.078   4.83499] 1 1.8349899999999826 False\n",
      "309 [-0.02     4.83528] 0 1.8352799999999823 False\n",
      "310 [0.078   4.83557] 1 1.835569999999982 False\n",
      "311 [-0.02     4.83586] 0 1.8358599999999816 False\n",
      "312 [0.078   4.83615] 1 1.8361499999999813 False\n",
      "313 [-0.02     4.83644] 0 1.836439999999981 False\n",
      "314 [0.078   4.83673] 1 1.8367299999999807 False\n",
      "315 [-0.02     4.83702] 0 1.8370199999999803 False\n",
      "316 [0.078   4.83731] 1 1.83730999999998 False\n",
      "317 [-0.02    4.8376] 0 1.8375999999999797 False\n",
      "318 [0.078   4.83789] 1 1.8378899999999794 False\n",
      "319 [-0.02     4.83818] 0 1.838179999999979 False\n",
      "320 [0.078   4.83847] 1 1.8384699999999787 False\n",
      "321 [-0.02     4.83876] 0 1.8387599999999784 False\n",
      "322 [0.078   4.83905] 1 1.839049999999978 False\n",
      "323 [-0.02     4.83934] 0 1.8393399999999778 False\n",
      "324 [0.078   4.83963] 1 1.8396299999999774 False\n",
      "325 [-0.02     4.83992] 0 1.8399199999999771 False\n",
      "326 [0.078   4.84021] 1 1.8402099999999768 False\n",
      "327 [-0.02    4.8405] 0 1.8404999999999765 False\n",
      "328 [0.078   4.84079] 1 1.8407899999999762 False\n",
      "329 [-0.02     4.84108] 0 1.8410799999999758 False\n",
      "330 [0.078   4.84137] 1 1.8413699999999755 False\n",
      "331 [-0.02     4.84166] 0 1.8416599999999752 False\n",
      "332 [0.078   4.84195] 1 1.8419499999999749 False\n",
      "333 [-0.02     4.84224] 0 1.8422399999999746 False\n",
      "334 [0.078   4.84253] 1 1.8425299999999742 False\n",
      "335 [-0.02     4.84282] 0 1.842819999999974 False\n",
      "336 [0.078   4.84311] 1 1.8431099999999736 False\n",
      "337 [-0.02    4.8434] 0 1.8433999999999733 False\n",
      "338 [0.078   4.84369] 1 1.843689999999973 False\n",
      "339 [-0.02     4.84398] 0 1.8439799999999726 False\n",
      "340 [0.078   4.84427] 1 1.8442699999999723 False\n",
      "341 [-0.02     4.84456] 0 1.844559999999972 False\n",
      "342 [0.078   4.84485] 1 1.8448499999999717 False\n",
      "343 [-0.02     4.84514] 0 1.8451399999999714 False\n",
      "344 [0.078   4.84543] 1 1.845429999999971 False\n",
      "345 [-0.02     4.84572] 0 1.8457199999999707 False\n",
      "346 [0.078   4.84601] 1 1.8460099999999704 False\n",
      "347 [-0.02    4.8463] 0 1.84629999999997 False\n",
      "348 [0.078   4.84659] 1 1.8465899999999698 False\n",
      "349 [-0.02     4.84688] 0 1.8468799999999694 False\n",
      "350 [0.078   4.84717] 1 1.8471699999999691 False\n",
      "351 [-0.02     4.84746] 0 1.8474599999999688 False\n",
      "352 [0.078   4.84775] 1 1.8477499999999685 False\n",
      "353 [-0.02     4.84804] 0 1.8480399999999682 False\n",
      "354 [0.078   4.84833] 1 1.8483299999999678 False\n",
      "355 [-0.02     4.84862] 0 1.8486199999999675 False\n",
      "356 [0.078   4.84891] 1 1.8489099999999672 False\n",
      "357 [-0.02    4.8492] 0 1.8491999999999669 False\n",
      "358 [0.078   4.84949] 1 1.8494899999999665 False\n",
      "359 [-0.02     4.84978] 0 1.8497799999999662 False\n",
      "360 [0.078   4.85007] 1 1.850069999999966 False\n",
      "361 [-0.02     4.85036] 0 1.8503599999999656 False\n",
      "362 [0.078   4.85065] 1 1.8506499999999653 False\n",
      "363 [-0.02     4.85094] 0 1.850939999999965 False\n",
      "364 [0.078   4.85123] 1 1.8512299999999646 False\n",
      "365 [-0.02     4.85152] 0 1.8515199999999643 False\n",
      "366 [0.078   4.85181] 1 1.851809999999964 False\n",
      "367 [-0.02    4.8521] 0 1.8520999999999637 False\n",
      "368 [0.078   4.85239] 1 1.8523899999999633 False\n",
      "369 [-0.02     4.85268] 0 1.852679999999963 False\n",
      "370 [0.078   4.85297] 1 1.8529699999999627 False\n",
      "371 [-0.02     4.85326] 0 1.8532599999999624 False\n",
      "372 [0.078   4.85355] 1 1.853549999999962 False\n",
      "373 [-0.02     4.85384] 0 1.8538399999999617 False\n",
      "374 [0.078   4.85413] 1 1.8541299999999614 False\n",
      "375 [-0.02     4.85442] 0 1.854419999999961 False\n",
      "376 [0.078   4.85471] 1 1.8547099999999608 False\n",
      "377 [-0.02   4.855] 0 1.8549999999999605 False\n",
      "378 [0.078   4.85529] 1 1.8552899999999601 False\n",
      "379 [-0.02     4.85558] 0 1.8555799999999598 False\n",
      "380 [0.078   4.85587] 1 1.8558699999999595 False\n",
      "381 [-0.02     4.85616] 0 1.8561599999999592 False\n",
      "382 [0.078   4.85645] 1 1.8564499999999589 False\n",
      "383 [-0.02     4.85674] 0 1.8567399999999585 False\n",
      "384 [0.078   4.85703] 1 1.8570299999999582 False\n",
      "385 [-0.02     4.85732] 0 1.857319999999958 False\n",
      "386 [0.078   4.85761] 1 1.8576099999999576 False\n",
      "387 [-0.02    4.8579] 0 1.8578999999999573 False\n",
      "388 [0.078   4.85819] 1 1.858189999999957 False\n",
      "389 [-0.02     4.85848] 0 1.8584799999999566 False\n",
      "390 [0.078   4.85877] 1 1.8587699999999563 False\n",
      "391 [-0.02     4.85906] 0 1.859059999999956 False\n",
      "392 [0.078   4.85935] 1 1.8593499999999556 False\n",
      "393 [-0.02     4.85964] 0 1.8596399999999553 False\n",
      "394 [0.078   4.85993] 1 1.859929999999955 False\n",
      "395 [-0.02     4.86022] 0 1.8602199999999547 False\n",
      "396 [0.078   4.86051] 1 1.8605099999999544 False\n",
      "397 [-0.02    4.8608] 0 1.860799999999954 False\n",
      "398 [0.078   4.86109] 1 1.8610899999999537 False\n",
      "399 [-0.02     4.86138] 0 1.8613799999999534 False\n",
      "400 [0.078   4.86167] 1 1.861669999999953 False\n",
      "401 [-0.02     4.86196] 0 1.8619599999999528 False\n",
      "402 [0.078   4.86225] 1 1.8622499999999524 False\n",
      "403 [-0.02     4.86254] 0 1.8625399999999521 False\n",
      "404 [0.078   4.86283] 1 1.8628299999999518 False\n",
      "405 [-0.02     4.86312] 0 1.8631199999999515 False\n",
      "406 [-0.118    4.86243] 0 1.8624299999999518 False\n",
      "407 [-0.02     4.86174] 1 1.8617399999999522 False\n",
      "408 [0.078   4.86203] 1 1.862029999999952 False\n",
      "409 [-0.02     4.86232] 0 1.8623199999999516 False\n",
      "410 [0.078   4.86261] 1 1.8626099999999512 False\n",
      "411 [-0.02    4.8629] 0 1.862899999999951 False\n",
      "412 [-0.118    4.86221] 0 1.8622099999999513 False\n",
      "413 [-0.02     4.86152] 1 1.8615199999999517 False\n",
      "414 [0.078   4.86181] 1 1.8618099999999513 False\n",
      "415 [-0.02    4.8621] 0 1.862099999999951 False\n",
      "416 [0.078   4.86239] 1 1.8623899999999507 False\n",
      "417 [-0.02     4.86268] 0 1.8626799999999504 False\n",
      "418 [-0.118    4.86199] 0 1.8619899999999507 False\n",
      "419 [-0.02    4.8613] 1 1.861299999999951 False\n",
      "420 [0.078   4.86159] 1 1.8615899999999508 False\n",
      "421 [-0.02     4.86188] 0 1.8618799999999505 False\n",
      "422 [0.078   4.86217] 1 1.8621699999999501 False\n",
      "423 [-0.02     4.86246] 0 1.8624599999999498 False\n",
      "424 [0.078   4.86275] 1 1.8627499999999495 False\n",
      "425 [-0.02     4.86304] 0 1.8630399999999492 False\n",
      "426 [-0.118    4.86235] 0 1.8623499999999495 False\n",
      "427 [-0.02     4.86166] 1 1.86165999999995 False\n",
      "428 [0.078   4.86195] 1 1.8619499999999496 False\n",
      "429 [-0.02     4.86224] 0 1.8622399999999493 False\n",
      "430 [0.078   4.86253] 1 1.862529999999949 False\n",
      "431 [-0.02     4.86282] 0 1.8628199999999486 False\n",
      "432 [-0.118    4.86213] 0 1.862129999999949 False\n",
      "433 [-0.02     4.86144] 1 1.8614399999999494 False\n",
      "434 [0.078   4.86173] 1 1.861729999999949 False\n",
      "435 [-0.02     4.86202] 0 1.8620199999999487 False\n",
      "436 [0.078   4.86231] 1 1.8623099999999484 False\n",
      "437 [-0.02    4.8626] 0 1.862599999999948 False\n",
      "438 [0.078   4.86289] 1 1.8628899999999478 False\n",
      "439 [-0.02     4.86318] 0 1.8631799999999474 False\n",
      "440 [-0.118    4.86249] 0 1.8624899999999478 False\n",
      "441 [-0.02    4.8618] 1 1.8617999999999482 False\n",
      "442 [0.078   4.86209] 1 1.8620899999999478 False\n",
      "443 [-0.02     4.86238] 0 1.8623799999999475 False\n",
      "444 [0.078   4.86267] 1 1.8626699999999472 False\n",
      "445 [-0.02     4.86296] 0 1.8629599999999469 False\n",
      "446 [-0.118    4.86227] 0 1.8622699999999472 False\n",
      "447 [-0.02     4.86158] 1 1.8615799999999476 False\n",
      "448 [0.078   4.86187] 1 1.8618699999999473 False\n",
      "449 [-0.02     4.86216] 0 1.862159999999947 False\n",
      "450 [0.078   4.86245] 1 1.8624499999999466 False\n",
      "451 [-0.02     4.86274] 0 1.8627399999999463 False\n",
      "452 [-0.118    4.86205] 0 1.8620499999999467 False\n",
      "453 [-0.02     4.86136] 1 1.861359999999947 False\n",
      "454 [0.078   4.86165] 1 1.8616499999999467 False\n",
      "455 [-0.02     4.86194] 0 1.8619399999999464 False\n",
      "456 [0.078   4.86223] 1 1.862229999999946 False\n",
      "457 [-0.02     4.86252] 0 1.8625199999999458 False\n",
      "458 [0.078   4.86281] 1 1.8628099999999455 False\n",
      "459 [-0.02    4.8631] 0 1.8630999999999451 False\n",
      "460 [-0.118    4.86241] 0 1.8624099999999455 False\n",
      "461 [-0.02     4.86172] 1 1.8617199999999459 False\n",
      "462 [0.078   4.86201] 1 1.8620099999999455 False\n",
      "463 [-0.02    4.8623] 0 1.8622999999999452 False\n",
      "464 [0.078   4.86259] 1 1.862589999999945 False\n",
      "465 [-0.02     4.86288] 0 1.8628799999999446 False\n",
      "466 [-0.118    4.86219] 0 1.862189999999945 False\n",
      "467 [-0.02    4.8615] 1 1.8614999999999453 False\n",
      "468 [0.078   4.86179] 1 1.861789999999945 False\n",
      "469 [-0.02     4.86208] 0 1.8620799999999447 False\n",
      "470 [0.078   4.86237] 1 1.8623699999999443 False\n",
      "471 [-0.02     4.86266] 0 1.862659999999944 False\n",
      "472 [-0.118    4.86197] 0 1.8619699999999444 False\n",
      "473 [-0.02     4.86128] 1 1.8612799999999448 False\n",
      "474 [0.078   4.86157] 1 1.8615699999999444 False\n",
      "475 [-0.02     4.86186] 0 1.8618599999999441 False\n",
      "476 [0.078   4.86215] 1 1.8621499999999438 False\n",
      "477 [-0.02     4.86244] 0 1.8624399999999435 False\n",
      "478 [0.078   4.86273] 1 1.8627299999999432 False\n",
      "479 [-0.02     4.86302] 0 1.8630199999999428 False\n",
      "480 [-0.118    4.86233] 0 1.8623299999999432 False\n",
      "481 [-0.02     4.86164] 1 1.8616399999999436 False\n",
      "482 [0.078   4.86193] 1 1.8619299999999432 False\n",
      "483 [-0.02     4.86222] 0 1.862219999999943 False\n",
      "484 [0.078   4.86251] 1 1.8625099999999426 False\n",
      "485 [-0.02    4.8628] 0 1.8627999999999423 False\n",
      "486 [-0.118    4.86211] 0 1.8621099999999426 False\n",
      "487 [-0.02     4.86142] 1 1.861419999999943 False\n",
      "488 [0.078   4.86171] 1 1.8617099999999427 False\n",
      "489 [-0.02   4.862] 0 1.8619999999999424 False\n",
      "490 [0.078   4.86229] 1 1.862289999999942 False\n",
      "491 [-0.02     4.86258] 0 1.8625799999999417 False\n",
      "492 [0.078   4.86287] 1 1.8628699999999414 False\n",
      "493 [-0.02     4.86316] 0 1.863159999999941 False\n",
      "494 [-0.118    4.86247] 0 1.8624699999999415 False\n",
      "495 [-0.02     4.86178] 1 1.8617799999999418 False\n",
      "496 [0.078   4.86207] 1 1.8620699999999415 False\n",
      "497 [-0.02     4.86236] 0 1.8623599999999412 False\n",
      "498 [0.078   4.86265] 1 1.8626499999999409 False\n",
      "499 [-0.02     4.86294] 0 1.8629399999999405 False\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.position = 3.0\n",
    "env.velocity = -1.0\n",
    "\n",
    "state = [env.velocity,env.position]\n",
    "S = [state] #States history for test\n",
    "for i in range(500):    \n",
    "    action = select_action(FloatTensor([state]))\n",
    "    a = action.data.numpy()[0,0]\n",
    "    state,reward,done,_ = env.step(a)\n",
    "    S.append(state)\n",
    "    print(i,state,a,reward,done)\n",
    "    if done:\n",
    "        print(\"out of bounds\")\n",
    "    #env.render()\n",
    "S = np.array(S) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the position vs. the steps for the test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'position')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc33aba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0,10])\n",
    "plt.plot(S[:,1]); plt.xlabel('steps'); plt.ylabel('position')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the policy (the action being taken) in different regions of the state space and overlay the movement of the ball in the test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afsar\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0xe01dda0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc2964a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Plotting the policy in the state space.\n",
    "x = np.linspace(0, 10, 50)\n",
    "v = np.linspace(-20, 20, 60)\n",
    "A = np.zeros((len(x),len(v)))\n",
    "for i,xi in enumerate(x):\n",
    "    for j,vj in enumerate(v):\n",
    "        A[i,j] = select_action(FloatTensor([[vj,xi]])).data.numpy()[0,0]\n",
    "plt.figure(3)\n",
    "plt.contourf(v,x,A,levels=[0.1,1]);\n",
    "plt.xlabel(\"v\"); plt.ylabel(\"x\")\n",
    "plt.scatter(S[:,0],S[:,1],c='r'); plt.plot(S[0,0],S[0,1],c = 'k', marker ='*'); plt.scatter(S[-1,0],S[-1,1],c = 'k', marker ='s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exercises\n",
    "\n",
    "1. What is the response of the neural network if the mass of the ball is changed in testing?\n",
    "2. Write a reinforcement learning program which learns a policy for every possible mass of the ball from 0.2 to 1.9 kgs.\n",
    "3. Write a reinforcement learning program which learns a policy for every possible integer reference point between 0 and 10.\n",
    "\n",
    "### Acknowledgements\n",
    "I would like to recognize the effort by Noushan and Abdullah at PIEAS Data Science Lab for their contributions in the code. We will also like to acknowledge the support from HEC and PIEAS in setting up PIEAS Data Science Lab under the Institutional Strengthening Program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
