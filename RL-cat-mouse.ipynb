{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [Anaconda3]",
      "language": "python",
      "name": "Python [Anaconda3]"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "RL.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foxtrotmike/RL-MagLev/blob/master/RL-cat-mouse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxjc8ExjqiV-",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning with Neural Networks: A Simple Tutorial \n",
        "\n",
        "By Dr. Fayyaz Minhas (all rights reserved)\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This is a very simple tutorial for neural networks based Q-learning. We use a simple example of a magnetic levitation system. The learning objectives are listed below:\n",
        "\n",
        "1. Development of an understanding of the concepts of Q-learning\n",
        "2. How can neural networks be used for approximation of the Q-function\n",
        "3. How to develop a solution for a new problem using reinforcement learning\n",
        "4. How to develop a custom environment\n",
        "5. Complete end to end implementation of neural Q-learning in pyTorch\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Reinforcement learning (RL) is a machine learning technique that lets an AI learn from its own experience. It has been around for quite a while now and has surfed the wave of neural networks and deep learning very well: See Alpha-Go, Alpha-Go Zero and Other DeepMind Projects. It also has the ability to lead to a general AI. \n",
        "\n",
        "To understand RL, it is interesting to understand the big picture first: Consider an agent that can interact with its environment by taking certain actions. The environment gives the agent a reward. As an example, consider a game of chess in which the player (agent) makes moves (an action) and receives a reward (win or lose). It is interesting to note that the reward can be delayed (till the end of the episode) and the agent may have no control over its environment which can change even without the actions of the player (e.g., by the action of the other player). Furthermore, the agent may have no knowledge of the internal working of the environment. The agent is to learn a \"policy\" that describes what actions to take in different states (board states in chess) so as to maximize its reward at the end of the episode (a single game). Think of reinforcement learning as a means of searching for the optimal policy. This is done using Q-learning.\n",
        "\n",
        "## Q-learning\n",
        "\n",
        "Consider an agent in state $s_t$ at time $t$. Assume that the agent takes an action $a_t$ and moves to state $s_{t+1}$ where it gets a reward $r_t$ for its action leading to state $s_{t+1}$. The \"quality\" of an action, i.e., how good or bad an action $a$ is in a particular state $s$, can be described by a Q-function $Q(s,a)$. If we know the Q-function, we can determine what is the optimal action in a given state. Thus, Q-function models the policy of the agent. We want to learn a Q-function that  maximizes the reward for the agent. In order to learn the Q-function, assume that we initialize the Q-function randomly, i.e., it will generate a random score for a given (state,action) pair. We would like to update the current value of the Q-function $Q(s_t,a_t)$ based on the reward $r_t$ of moving to state $s_{t+1}$ from $s_t$ by taking action $a_t$: if the reward of being in state $s_t$ is high, the value of $Q(s_t,a_t)$ should be increased and vice-versa. Additionally, if the state $s_{t+1}$ is a \"favorable\" state, i.e., actions in state $s_{t+1}$ can lead to highly favorable situations then the value of $Q(s_t,a_t)$ should be increased and vice-versa. The favorability or goodness of the state $s_{t+1}$ (tecnically called utility $U(s_{t+1})$) can be described by the maximum Q-value of the best action in that state, i.e., $U(s_{t+1})=max_a Q(s_{t+1},a)$. This leads us to update the Q-function in the following manner: $Q(s_t,a_t)^{new} \\leftarrow (1-\\alpha)Q(s_t,a_t)+\\alpha(r_t+\\gamma max_a Q(s_{t+1},a)$. Here, $\\alpha \\in [0,1]$ and $\\gamma \\in (0,1)$, called learning rate and discount factor, respectively, are two constants. Note that the update is a weighted combination of the previous value of $Q$ and the information obtained as a consequence of the agent's action. Before we go into the detail of these constants, try to convince yourself that the update equation indeed follows the requirements for the update above, i.e., the Q-value is updated based on:\n",
        "\n",
        "1. Its previous value $Q(s_t,a_t)$: Higher the previous value, higher the new value\n",
        "2. Reward $r_t$: Higher the reward, higher the new value\n",
        "3. Utility of the state $U(s_{t+1})=max_a Q(s_{t+1},a)$: Higher the utility, higher the new  value\n",
        "\n",
        "### Role of learning rate\n",
        "\n",
        "The learning rate controls how much to add to the previous Q-value: If the old values are more reliable in comparison to the one obtained based on current reward and utility of the next state which can happen in noisy reward scenarios or uncertain utilities, the value of the learning rate should be set to low and high otherwise. However, if $\\alpha=0$, not learning takes place. In fully deterministic environments, a learning rate of 1.0 is optimal since we know that the information being received in response to an action does not contain any noise or uncertainty. When the problem is stochastic, the algorithm converges under some technical conditions on the learning rate that require it to decrease to zero. \n",
        "\n",
        "### Role of discount factor\n",
        "\n",
        "The discount factor controls the impact of the utility of the next state, i.e., it controls the effect of future rewards, since the utility value $U(s_{t+1})=max_a Q(s_{t+1},a)$ is based on Q-value of the state $s_{t+1}$ which is, in turn, based on rewards $r_{t+1}$ and $U(s_{t+2})$ and so on. To understand it better, note that the update equation for the Q-value is a recursive term and it involves $\\gamma U(s_{t+1})$ and $U(s_{t+1})$ will involve $\\gamma U(s_{t+2})$ and so on. Thus, the role of future utilies can be written as $\\gamma^k U(s_{t+k})$. As a consequence, the larger the value of $gamma$, the more terms of future utilities will be included in the update of the current Q-value. Setting $gamma$ close to zero, would make the agent greedy as it will consider only the current reward and may not learn an effective policy that will maximize the future rewards. This is analogous to a chess player who goes on a \"killing spree\" by killing opponent's pieces as and when possible without considering that allowing an opponent's piece to be on the board can lead to better states in the future.\n",
        "\n",
        "### Algorithm Description\n",
        "\n",
        "Now one might ask the question, how does this work in practice? This is how.\n",
        "\n",
        "One starts off with a random Q-value function. In a given episode, the agent is \"spawned\" in an intial state in which the agent makes a move based on the Q-value, i.e., pick an action based on the highest Q-value across all possible actions in that state. This will change the state of the agent and the agent will receive a reward. Based on this, the agent will update the Q-value of the previous state. Then the agent will pick another action and keep updating it until either the end or terminal state is reached or the user defined threshold on the number of steps in an episode expires. This is repeated for a number of episodes until the Q-values stop changing. It is important to note that if all the updates are based on optimal Q-values, the agent might not explore all possible states or actions. As a consequence, an $\\epsilon$-greedy policy is typically employed in whcih an agent takes random actions at a given state, instead of consulting the Q-value function, with a small probability $\\epsilon \\ge 0$. Typically, $\\epsilon$ is initialized to a high value (0.9) and gradually decreased over episodes or number of total steps. Similarly, the learning rate is also \"annealed\" or decreased gradually as the Q-function becomes more \"learned\" over episodes. The discount factor is also increased gradually to model the improved certainty of future rewards over episodes. \n",
        "\n",
        "### Implementation using Q-tables or Neural Networks\n",
        "\n",
        "It is interesting to note that $Q(s,a)$ is a function that, gives a value for a given state and action as input. We want to learn this function using the Q-learning update equation. If the states and actions are discrete and finite, then we can use a simple table that stores the Q-value of a given state-action pair. However, if the states or actions are continuous or infinite, this is not possible. For such cases, we can use neural networks.\n",
        "\n",
        "Neural networks are universal function approximators that can approximate the value of a function $f(x)$ based on its inputs $x$. This is done by minimizing an error or loss function between the output of a neural network $F(x)$ and the target value $f(x)$ over a set of training examples by updating the weight parameters of the network. \n",
        "\n",
        "We can use a neural network to \"store\" the Q-table or approximate the Q-function. Think of it this way: Instead of getting the Q-values from the Q-table, we will get them from the neural network. However, the issue with RL is that we do not know the optimal Q-function before hand and it must be learned on the fly. Thus, we will be simultaneously learning the Q-function and the neural network that approximates it. This is how it can be done.\n",
        "\n",
        "We can start off the agent in a certain state $s_t$ and use the neural network to give the Q-value for a given action by giving the neural network the current state-action pair or by using the $\\epsilon$ greedy policy that allows the agent to take random actions too. As a consequence, we will get a new Q value based on the update equation. We can store the current state-action pair and its updated Q-value in a memory or \"replay queue\" which can store up to a certain number of such training examples in it. We then do a certain number of training epochs over randomly selected input-output pairs where the input is the state-action pair and the output is the corresponding q-value. Repeating this process over a number of episodes will train the neural network to approximate the Q-function.\n",
        "\n",
        "However, we are going to start learning about it by controlling the position of a metallic ball using a magnet.\n",
        "\n",
        "## Magnetic Levitation System\n",
        "\n",
        "Consider a metallic ball of mass $m$ at positiopn $x_0$ moving with velocity $v_0$ at time $t$ which is acted upon by gravity in the downwards direction and a fixed force $F$ in the upward direction resulting from an electromagnet that can be switched on (action A = 1) or off (action A = 0) to control the position of the ball along the vertical. Thus, the net force $aF-mg$ causes acceleration $a=(AF-mg)/m$ in the mass. In a given time step $dt = 0.01$, the  velocity of the ball at time $t+dt$ becomoes $v = v_0+a*dt$ and its new position will be $x = x_0+v_dt+0.5a(dt)^2$. Thus far we have modeled the physics of the problem and the state of the system can be described by a vector $(v,x)$.\n",
        "\n",
        "The objective here is to develop an intelligent agent that can switch the electromagnet on or off so that the ball reaches a certain reference position. For this purpose, the environment gives the agent a reward in a given state based on how close the current position is to the target, i.e., $r=-|x-x_{ref}|$. \n",
        "\n",
        "We can implement this system as an Open AI Gym Environment that allows us to model the physics, visualize, and get rewards. The most important function is the step function which takes in an action and retursn the state and rewrard based on the system's current state. The complete implementation is given below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D7T9tWxqiWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Created on Mon Aug 13 15:22:11 2018\n",
        "A simple example of a custom gym environment for a magnetic levitation system\n",
        "Imagine an iron ball of mass m placed at a certain location along the y-axis\n",
        "and a magentic force F pulling it up. How can we control its position?\n",
        "@author: Fayyaz Minhas, Noushan and Abdullah\n",
        "\"\"\"\n",
        "import gym\n",
        "from gym import spaces\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "class MagLevEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}    \n",
        "    \n",
        "    GRAVITY = 9.8\n",
        "    FORCE = GRAVITY*2.0 # force is twice as string as gravity (2 Kg can be controlled)\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        self.__version__ = \"0.0.1.0\"\n",
        "        logging.info(\"MAGLevEnv - Version {}\".format(self.__version__))\n",
        "        \n",
        "        self.timestep = 0.01 #time step in every action\n",
        "        self.mass = 1.0\n",
        "        \n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Box(np.array([-20,-0.1]), np.array([+20, 10]), dtype=np.float32)\n",
        "        \n",
        "        self.lastAction = 0\n",
        "\n",
        "        self.referencepoint = 5.0\n",
        "        \n",
        "        self.reset()\n",
        "        \n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action :\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        obs, reward, episode_over, info : tuple\n",
        "            ob (object) :\n",
        "                an environment-specific object representing your observation of\n",
        "                the environment.\n",
        "            reward (float) :\n",
        "                amount of reward achieved by the previous action. The scale\n",
        "                varies between environments, but the goal is always to increase\n",
        "                your total reward.\n",
        "            episode_over (bool) :\n",
        "                whether it's time to reset the environment again. Most (but not\n",
        "                all) tasks are divided up into well-defined episodes, and done\n",
        "                being True indicates the episode has terminated. (For example,\n",
        "                perhaps the pole tipped too far, or you lost your last life.)\n",
        "            info (dict) :\n",
        "                 diagnostic information useful for debugging. It can sometimes\n",
        "                 be useful for learning (for example, it might contain the raw\n",
        "                 probabilities behind the environment's last state change).\n",
        "                 However, official evaluations of your agent are not allowed to\n",
        "                 use this for learning.\n",
        "        \"\"\"\n",
        "        self._take_action(action)\n",
        "        self.lastAction = action\n",
        "        done = False\n",
        "        reward = self._get_reward()\n",
        "        obs = self._get_state()\n",
        "        \n",
        "        if not self.observation_space.contains(obs):\n",
        "            done = True\n",
        "        \n",
        "        return obs, reward, done, {}\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state of the environment and returns an initial observation.\n",
        "        Returns\n",
        "        -------\n",
        "        observation (object): the initial observation of the space.\n",
        "        \"\"\"\n",
        "        \n",
        "        x = random.randint(0,10)\n",
        "        v = (2*np.random.rand()-1)*20\n",
        "        \n",
        "        self.acceleration = 0\n",
        "        self.velocity = v\n",
        "        self.position = x\n",
        "        \n",
        "        \n",
        "        return self._get_state()\n",
        "\n",
        "    def render(self, figid = 0):\n",
        "        \"\"\"\n",
        "        Shows a ball with the position indicated by the position. Velocity is \n",
        "        proportional to the size of the ball. The current action is shown in \n",
        "        color of the ball with blue indicating no upward force and red if the\n",
        "        force is active.\n",
        "        \"\"\"\n",
        "        \n",
        "        plt.figure(figid)\n",
        "        r = np.max((0.3,np.abs(self.velocity)/10.0))\n",
        "        c = 'b'\n",
        "        if self.lastAction:\n",
        "            c = 'r'\n",
        "        \n",
        "        \n",
        "        circle = plt.Circle((0,self.position), radius= r, color = c)\n",
        "        \n",
        "        ax=plt.gca()\n",
        "        ax.clear()\n",
        "        ax.add_patch(circle)\n",
        "        plt.axis('scaled')\n",
        "        plt.xlim(-10,10)\n",
        "        plt.ylim(-1,11)\n",
        "        plt.plot([-10,10],[self.referencepoint]*2)\n",
        "        plt.plot([-10,10],[0]*2)\n",
        "        plt.plot([-10,10],[10]*2)\n",
        "        plt.pause(0.00001)\n",
        "        plt.show()\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        \"\"\"\n",
        "        Model the effect of the action taken and update state.\n",
        "        Simple modeling of physics.\n",
        "        \"\"\"\n",
        "        \n",
        "        v0 = self.velocity\n",
        "        x0 = self.position   \n",
        "\n",
        "        a = ( ( action*MagLevEnv.FORCE / self.mass ) - MagLevEnv.GRAVITY )\n",
        "         \n",
        "        dv = ( a * self.timestep )\n",
        "        v = v0 + dv\n",
        "        dx = ( v0 * self.timestep ) + 0.5 * (a * self.timestep**2) \n",
        "        x = x0 + dx\n",
        "    \n",
        "        \n",
        "        self.acceleration = a\n",
        "        self.velocity = v\n",
        "        self.position = x\n",
        "        \n",
        "            \n",
        "    def _get_state(self):\n",
        "        \n",
        "        \"\"\"Get the observation.\"\"\"\n",
        "        \n",
        "        obs = np.asarray(list((self.velocity,self.position)))\n",
        "        return obs\n",
        "            \n",
        "\n",
        "    def _get_reward(self):\n",
        "        \"\"\"\n",
        "        Reward function.\n",
        "        \"\"\"\n",
        "        state = self._get_state()\n",
        "        reward =  float(-np.abs(state[1]-self.referencepoint))#*float(np.abs(next_state[0])<0.1)\n",
        "        if np.abs(state[1]-self.referencepoint)<0.5:\n",
        "            reward+=2.0\n",
        "        if not self.observation_space.contains(state):\n",
        "            reward-=1.0            \n",
        "        return reward\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"Mass: \"+str(self.mass)+\" Ref: \"+str(self.referencepoint)+\" State: \"+str(self._get_state())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SthqUu2jqiWD",
        "colab_type": "text"
      },
      "source": [
        "One can now create and environment and observe the effects of the actions as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWGBEBvxqiWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "87508d9a-865b-49a4-8427-070ff47949e1"
      },
      "source": [
        "env = MagLevEnv()\n",
        "print(env)\n",
        "env.step(1.0)\n",
        "print(env)\n",
        "env.render()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mass: 1.0 Ref: 5.0 State: [15.14932627  2.        ]\n",
            "Mass: 1.0 Ref: 5.0 State: [15.24732627  2.15198326]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAADoCAYAAAAOu0RSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PBJrR1TqiWJ",
        "colab_type": "text"
      },
      "source": [
        "Below is the complete implementation of a Neural Q-learning algorithm. A neural Q-learning program will consist of three parts: environment, neural network and Q-learning. We have already implemented the environemtn. We use a simple neural network with 2 inputs (corresponding to the two state variables), 200 hidden neurons with ReLU activations and 2 outputs (corresponding to Q-values for each action). For details, see the neural network class implementation below.  A single episode (executed by the run_episode function) consists of 500 steps in whcih the environment is initialized (reset) and the actions are determined using an epsilon greedy policy with annealed epsilon values together with the neural network (see the select_action function). At each step, the neural network picks up 64 examples at random and uses them to learn the Q-function (see the learn function). The number of episodes is set to 70. It is important to note that the only information of the environment visible to neural network is the one given by the environment (position, velocity and reward). The internal workings of the environment are not visible to Q-learning. We have a Q-learning rate of 1.0 and a discount factor of 0.98 since the environment is deterministric. The target position is 5.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIPFz8X_qiWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17ec3b8a-47f9-4ce3-c6b4-2ac7bfb2817a"
      },
      "source": [
        "\"\"\"\n",
        "Created on Mon Aug 13 15:22:11 2018\n",
        "A simple example of a magnetic levitation system controlled using Reinforcement Learning\n",
        "@author: Fayyaz Minhas\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "#from maglevEnv import MagLevEnv\n",
        "import numpy as np\n",
        "\n",
        "# hyper parameters\n",
        "EPISODES = 70  # number of episodes\n",
        "EPS_START = 0.9  # e-greedy threshold start value\n",
        "EPS_END = 0.01 # e-greedy threshold end value\n",
        "EPS_DECAY = 1000  # e-greedy threshold decay\n",
        "GAMMA = 0.98  # Q-learning discount factor\n",
        "LR = 0.005  # NN optimizer learning rate\n",
        "BATCH_SIZE = 64  # Q-learning batch size\n",
        "\n",
        "# if gpu is to be used\n",
        "use_cuda = torch.cuda.is_available()\n",
        "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
        "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
        "Tensor = FloatTensor\n",
        "\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.memory.append(transition)\n",
        "        if len(self.memory) > self.capacity:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        nn.Module.__init__(self)\n",
        "        self.l1 = nn.Linear(2, 200)\n",
        "        self.l3 = nn.Linear(200, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = (self.l3(x))\n",
        "        return x\n",
        "\n",
        "env = MagLevEnv()\n",
        "env.referencepoint = 5.0\n",
        "\n",
        "model = Network()\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "memory = ReplayMemory(10000)\n",
        "optimizer = optim.Adam(model.parameters(), LR)\n",
        "steps_done = 0\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return LongTensor([[random.randrange(2)]])\n",
        "\n",
        "\n",
        "def run_episode(e, environment):\n",
        "    ref = environment.referencepoint\n",
        "    state = environment.reset()    \n",
        "    steps = 0\n",
        "    while True:\n",
        "        steps += 1\n",
        "        \n",
        "        action = select_action(FloatTensor([state]))\n",
        "        a = action.data.numpy()[0,0]\n",
        "        next_state, reward, done, _ = environment.step(a)\n",
        "\n",
        "\n",
        "\n",
        "        memory.push((FloatTensor([state]),\n",
        "                     action,  # action is already a tensor\n",
        "                     FloatTensor([next_state]),\n",
        "                     FloatTensor([reward])))\n",
        "\n",
        "        \n",
        "        learn()\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if steps > 500:\n",
        "            print(\"Episode %s Final Position Error %s \" %(e, np.abs(next_state[1]-ref)))\n",
        "            episode_durations.append(np.abs(next_state[1]-ref))\n",
        "            #plotError()\n",
        "            \n",
        "            break\n",
        "\n",
        "\n",
        "def learn():\n",
        "    global GAMMA\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    \n",
        "    # random transition batch is taken from experience replay memory\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
        "\n",
        "    batch_state = Variable(torch.cat(batch_state))\n",
        "    batch_action = Variable(torch.cat(batch_action))\n",
        "    batch_reward = Variable(torch.cat(batch_reward))\n",
        "    batch_next_state = Variable(torch.cat(batch_next_state))\n",
        "\n",
        "    # current Q values are estimated by NN for all actions\n",
        "    current_q_values = model(batch_state).gather(1, batch_action)\n",
        "    # expected Q values are estimated from actions which gives maximum Q value\n",
        "    max_next_q_values = model(batch_next_state).detach().max(1)[0]\n",
        "        \n",
        "\n",
        "    expected_q_values = batch_reward + (GAMMA * max_next_q_values)\n",
        "\n",
        "    # loss is measured from error between current and newly expected Q values\n",
        "    loss = F.smooth_l1_loss(current_q_values, expected_q_values.view(BATCH_SIZE,1))\n",
        "\n",
        "    # backpropagation of loss to NN\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "def plotError():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.FloatTensor(episode_durations)\n",
        "    plt.title(str(steps_done))\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Final Position Error')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # take 100 episode averages and plot them too\n",
        "    H = 10\n",
        "    if len(durations_t) >= H:\n",
        "        means = durations_t.unfold(0, H, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(H-1), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "\n",
        "for e in range(EPISODES):\n",
        "    run_episode(e, env)\n",
        "\n",
        "print('Complete')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 0 Final Position Error 51.62836626100896 \n",
            "Episode 1 Final Position Error 42.28925191122959 \n",
            "Episode 2 Final Position Error 11.227288914390599 \n",
            "Episode 3 Final Position Error 3.2857064343723112 \n",
            "Episode 4 Final Position Error 6.6461308602105404 \n",
            "Episode 5 Final Position Error 0.1398615151686764 \n",
            "Episode 6 Final Position Error 0.1874361832858158 \n",
            "Episode 7 Final Position Error 0.8681092874446117 \n",
            "Episode 8 Final Position Error 0.717691890576174 \n",
            "Episode 9 Final Position Error 0.5847177178683669 \n",
            "Episode 10 Final Position Error 0.24581563662457828 \n",
            "Episode 11 Final Position Error 0.10109746595428248 \n",
            "Episode 12 Final Position Error 0.1053704050960631 \n",
            "Episode 13 Final Position Error 0.1645259296520587 \n",
            "Episode 14 Final Position Error 0.016486754857054642 \n",
            "Episode 15 Final Position Error 0.05244507920883912 \n",
            "Episode 16 Final Position Error 0.012551293276040987 \n",
            "Episode 17 Final Position Error 0.27965957534284325 \n",
            "Episode 18 Final Position Error 0.052441884682780326 \n",
            "Episode 19 Final Position Error 5.9526798907167855 \n",
            "Episode 20 Final Position Error 5.721421057601104 \n",
            "Episode 21 Final Position Error 0.08478510003845852 \n",
            "Episode 22 Final Position Error 0.060970573136652995 \n",
            "Episode 23 Final Position Error 0.2881573983143495 \n",
            "Episode 24 Final Position Error 0.10236582491917989 \n",
            "Episode 25 Final Position Error 0.32687595258501023 \n",
            "Episode 26 Final Position Error 0.24253289026849156 \n",
            "Episode 27 Final Position Error 0.03795468172402661 \n",
            "Episode 28 Final Position Error 0.18145203738499127 \n",
            "Episode 29 Final Position Error 0.30769755843498636 \n",
            "Episode 30 Final Position Error 0.13616223217458234 \n",
            "Episode 31 Final Position Error 0.0905562137171474 \n",
            "Episode 32 Final Position Error 0.07675197440434545 \n",
            "Episode 33 Final Position Error 0.7426749965323607 \n",
            "Episode 34 Final Position Error 0.07565423324131526 \n",
            "Episode 35 Final Position Error 0.07034409481869641 \n",
            "Episode 36 Final Position Error 0.10045921321602247 \n",
            "Episode 37 Final Position Error 133.8353635817059 \n",
            "Episode 38 Final Position Error 0.11355315959777723 \n",
            "Episode 39 Final Position Error 6.91339864790263 \n",
            "Episode 40 Final Position Error 0.10156690212661879 \n",
            "Episode 41 Final Position Error 0.08815604741938898 \n",
            "Episode 42 Final Position Error 0.10727930167093547 \n",
            "Episode 43 Final Position Error 0.1008467539146567 \n",
            "Episode 44 Final Position Error 0.09026767044648754 \n",
            "Episode 45 Final Position Error 0.06921788745385626 \n",
            "Episode 46 Final Position Error 35.31192106519086 \n",
            "Episode 47 Final Position Error 0.12723436056602289 \n",
            "Episode 48 Final Position Error 3.4516105673169655 \n",
            "Episode 49 Final Position Error 0.14012298030303505 \n",
            "Episode 50 Final Position Error 0.12331895245467717 \n",
            "Episode 51 Final Position Error 0.1070174383046485 \n",
            "Episode 52 Final Position Error 4.4258631716979195 \n",
            "Episode 53 Final Position Error 14.657253883047272 \n",
            "Episode 54 Final Position Error 0.1428800847596552 \n",
            "Episode 55 Final Position Error 0.1204166897960075 \n",
            "Episode 56 Final Position Error 0.09447235756919081 \n",
            "Episode 57 Final Position Error 0.10998467061968054 \n",
            "Episode 58 Final Position Error 0.12482614228045019 \n",
            "Episode 59 Final Position Error 0.11125099110378223 \n",
            "Episode 60 Final Position Error 0.09696183966714145 \n",
            "Episode 61 Final Position Error 9.923278834822565 \n",
            "Episode 62 Final Position Error 0.09274962603627035 \n",
            "Episode 63 Final Position Error 0.12784154580452878 \n",
            "Episode 64 Final Position Error 0.7038697853794034 \n",
            "Episode 65 Final Position Error 0.11811860824591847 \n",
            "Episode 66 Final Position Error 0.14585748540048726 \n",
            "Episode 67 Final Position Error 0.1018508276808543 \n",
            "Episode 68 Final Position Error 0.11575238808445754 \n",
            "Episode 69 Final Position Error 0.15532049583117846 \n",
            "Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbMODM0rqiWM",
        "colab_type": "text"
      },
      "source": [
        "Once the learning is complete, we can test the performance of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DupJX5edqiWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "92297a24-52f8-4612-aa29-f1f31a0b6628"
      },
      "source": [
        "state = env.reset()\n",
        "env.position = 3.0\n",
        "env.velocity = -1.0\n",
        "\n",
        "state = [env.velocity,env.position]\n",
        "S = [state] #States history for test\n",
        "for i in range(500):    \n",
        "    action = select_action(FloatTensor([state]))\n",
        "    a = action.data.numpy()[0,0]\n",
        "    state,reward,done,_ = env.step(a)\n",
        "    S.append(state)\n",
        "    print(i,state,a,reward,done)\n",
        "    if done:\n",
        "        print(\"out of bounds\")\n",
        "    #env.render()\n",
        "S = np.array(S) \n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [-0.902    2.99049] 1 -2.00951 False\n",
            "1 [-0.804    2.98196] 1 -2.01804 False\n",
            "2 [-0.706    2.97441] 1 -2.0255900000000002 False\n",
            "3 [-0.608    2.96784] 1 -2.03216 False\n",
            "4 [-0.51     2.96225] 1 -2.0377500000000004 False\n",
            "5 [-0.412    2.95764] 1 -2.0423600000000004 False\n",
            "6 [-0.314    2.95401] 1 -2.04599 False\n",
            "7 [-0.216    2.95136] 1 -2.0486400000000002 False\n",
            "8 [-0.118    2.94969] 1 -2.05031 False\n",
            "9 [-0.02   2.949] 1 -2.051 False\n",
            "10 [0.078   2.94929] 1 -2.05071 False\n",
            "11 [0.176   2.95056] 1 -2.04944 False\n",
            "12 [0.274   2.95281] 1 -2.04719 False\n",
            "13 [0.372   2.95604] 1 -2.04396 False\n",
            "14 [0.47    2.96025] 1 -2.03975 False\n",
            "15 [0.568   2.96544] 1 -2.0345600000000004 False\n",
            "16 [0.666   2.97161] 1 -2.0283900000000004 False\n",
            "17 [0.764   2.97876] 1 -2.02124 False\n",
            "18 [0.862   2.98689] 1 -2.01311 False\n",
            "19 [0.96  2.996] 1 -2.004 False\n",
            "20 [1.058   3.00609] 1 -1.99391 False\n",
            "21 [1.156   3.01716] 1 -1.98284 False\n",
            "22 [1.254   3.02921] 1 -1.97079 False\n",
            "23 [1.352   3.04224] 1 -1.95776 False\n",
            "24 [1.45    3.05625] 1 -1.94375 False\n",
            "25 [1.548   3.07124] 1 -1.92876 False\n",
            "26 [1.646   3.08721] 1 -1.9127900000000002 False\n",
            "27 [1.744   3.10416] 1 -1.8958400000000002 False\n",
            "28 [1.842   3.12209] 1 -1.87791 False\n",
            "29 [1.94  3.141] 1 -1.859 False\n",
            "30 [2.038   3.16089] 1 -1.8391099999999998 False\n",
            "31 [2.136   3.18176] 1 -1.8182399999999999 False\n",
            "32 [2.234   3.20361] 1 -1.7963899999999997 False\n",
            "33 [2.332   3.22644] 1 -1.7735599999999998 False\n",
            "34 [2.43    3.25025] 1 -1.7497499999999997 False\n",
            "35 [2.332   3.27406] 0 -1.7259399999999996 False\n",
            "36 [2.234   3.29689] 0 -1.7031099999999997 False\n",
            "37 [2.332   3.31972] 1 -1.6802799999999998 False\n",
            "38 [2.234   3.34255] 0 -1.6574499999999999 False\n",
            "39 [2.136  3.3644] 0 -1.6355999999999997 False\n",
            "40 [2.234   3.38625] 1 -1.6137499999999996 False\n",
            "41 [2.136  3.4081] 0 -1.5918999999999994 False\n",
            "42 [2.234   3.42995] 1 -1.5700499999999993 False\n",
            "43 [2.136  3.4518] 0 -1.5481999999999991 False\n",
            "44 [2.038   3.47267] 0 -1.5273299999999992 False\n",
            "45 [2.136   3.49354] 1 -1.5064599999999992 False\n",
            "46 [2.038   3.51441] 0 -1.4855899999999993 False\n",
            "47 [2.136   3.53528] 1 -1.4647199999999994 False\n",
            "48 [2.038   3.55615] 0 -1.4438499999999994 False\n",
            "49 [2.136   3.57702] 1 -1.4229799999999995 False\n",
            "50 [2.038   3.59789] 0 -1.4021099999999995 False\n",
            "51 [2.136   3.61876] 1 -1.3812399999999996 False\n",
            "52 [2.038   3.63963] 0 -1.3603699999999996 False\n",
            "53 [1.94    3.65952] 0 -1.3404799999999994 False\n",
            "54 [2.038   3.67941] 1 -1.3205899999999993 False\n",
            "55 [1.94   3.6993] 0 -1.300699999999999 False\n",
            "56 [1.842   3.71821] 0 -1.281789999999999 False\n",
            "57 [1.94    3.73712] 1 -1.2628799999999991 False\n",
            "58 [1.842   3.75603] 0 -1.2439699999999991 False\n",
            "59 [1.744   3.77396] 0 -1.226039999999999 False\n",
            "60 [1.842   3.79189] 1 -1.2081099999999987 False\n",
            "61 [1.744   3.80982] 0 -1.1901799999999985 False\n",
            "62 [1.646   3.82677] 0 -1.1732299999999984 False\n",
            "63 [1.744   3.84372] 1 -1.1562799999999984 False\n",
            "64 [1.646   3.86067] 0 -1.1393299999999984 False\n",
            "65 [1.744   3.87762] 1 -1.1223799999999984 False\n",
            "66 [1.646   3.89457] 0 -1.1054299999999984 False\n",
            "67 [1.548   3.91054] 0 -1.0894599999999985 False\n",
            "68 [1.646   3.92651] 1 -1.0734899999999987 False\n",
            "69 [1.548   3.94248] 0 -1.057519999999999 False\n",
            "70 [1.45    3.95747] 0 -1.0425299999999988 False\n",
            "71 [1.548   3.97246] 1 -1.0275399999999988 False\n",
            "72 [1.45    3.98745] 0 -1.0125499999999987 False\n",
            "73 [1.352   4.00146] 0 -0.9985399999999984 False\n",
            "74 [1.45    4.01547] 1 -0.9845299999999986 False\n",
            "75 [1.352   4.02948] 0 -0.9705199999999987 False\n",
            "76 [1.254   4.04251] 0 -0.9574899999999991 False\n",
            "77 [1.352   4.05554] 1 -0.9444599999999994 False\n",
            "78 [1.254   4.06857] 0 -0.9314299999999998 False\n",
            "79 [1.352  4.0816] 1 -0.9184000000000001 False\n",
            "80 [1.254   4.09463] 0 -0.9053700000000005 False\n",
            "81 [1.156   4.10668] 0 -0.8933200000000001 False\n",
            "82 [1.254   4.11873] 1 -0.8812699999999998 False\n",
            "83 [1.156   4.13078] 0 -0.8692199999999994 False\n",
            "84 [1.058   4.14185] 0 -0.8581499999999993 False\n",
            "85 [1.156   4.15292] 1 -0.8470799999999992 False\n",
            "86 [1.058   4.16399] 0 -0.836009999999999 False\n",
            "87 [1.156   4.17506] 1 -0.8249399999999989 False\n",
            "88 [1.058   4.18613] 0 -0.8138699999999988 False\n",
            "89 [0.96    4.19622] 0 -0.8037799999999988 False\n",
            "90 [1.058   4.20631] 1 -0.7936899999999989 False\n",
            "91 [0.96   4.2164] 0 -0.783599999999999 False\n",
            "92 [1.058   4.22649] 1 -0.773509999999999 False\n",
            "93 [0.96    4.23658] 0 -0.7634199999999991 False\n",
            "94 [0.862   4.24569] 0 -0.7543099999999994 False\n",
            "95 [0.96   4.2548] 1 -0.7451999999999996 False\n",
            "96 [0.862   4.26391] 0 -0.7360899999999999 False\n",
            "97 [0.96    4.27302] 1 -0.7269800000000002 False\n",
            "98 [0.862   4.28213] 0 -0.7178700000000005 False\n",
            "99 [0.764   4.29026] 0 -0.70974 False\n",
            "100 [0.862   4.29839] 1 -0.7016099999999996 False\n",
            "101 [0.764   4.30652] 0 -0.6934799999999992 False\n",
            "102 [0.862   4.31465] 1 -0.6853499999999988 False\n",
            "103 [0.764   4.32278] 0 -0.6772199999999984 False\n",
            "104 [0.862   4.33091] 1 -0.669089999999998 False\n",
            "105 [0.764   4.33904] 0 -0.6609599999999975 False\n",
            "106 [0.666   4.34619] 0 -0.6538099999999973 False\n",
            "107 [0.764   4.35334] 1 -0.6466599999999971 False\n",
            "108 [0.666   4.36049] 0 -0.6395099999999969 False\n",
            "109 [0.764   4.36764] 1 -0.6323599999999967 False\n",
            "110 [0.666   4.37479] 0 -0.6252099999999965 False\n",
            "111 [0.764   4.38194] 1 -0.6180599999999963 False\n",
            "112 [0.666   4.38909] 0 -0.6109099999999961 False\n",
            "113 [0.568   4.39526] 0 -0.6047399999999961 False\n",
            "114 [0.666   4.40143] 1 -0.598569999999996 False\n",
            "115 [0.568  4.4076] 0 -0.592399999999996 False\n",
            "116 [0.666   4.41377] 1 -0.586229999999996 False\n",
            "117 [0.568   4.41994] 0 -0.580059999999996 False\n",
            "118 [0.666   4.42611] 1 -0.573889999999996 False\n",
            "119 [0.568   4.43228] 0 -0.567719999999996 False\n",
            "120 [0.666   4.43845] 1 -0.561549999999996 False\n",
            "121 [0.568   4.44462] 0 -0.555379999999996 False\n",
            "122 [0.47    4.44981] 0 -0.5501899999999962 False\n",
            "123 [0.568 4.455] 1 -0.5449999999999964 False\n",
            "124 [0.47    4.46019] 0 -0.5398099999999966 False\n",
            "125 [0.568   4.46538] 1 -0.5346199999999968 False\n",
            "126 [0.47    4.47057] 0 -0.529429999999997 False\n",
            "127 [0.568   4.47576] 1 -0.5242399999999972 False\n",
            "128 [0.47    4.48095] 0 -0.5190499999999973 False\n",
            "129 [0.568   4.48614] 1 -0.5138599999999975 False\n",
            "130 [0.47    4.49133] 0 -0.5086699999999977 False\n",
            "131 [0.372   4.49554] 0 -0.5044599999999981 False\n",
            "132 [0.47    4.49975] 1 -0.5002499999999985 False\n",
            "133 [0.372   4.50396] 0 1.503960000000001 False\n",
            "134 [0.47    4.50817] 1 1.5081700000000007 False\n",
            "135 [0.372   4.51238] 0 1.5123800000000003 False\n",
            "136 [0.47    4.51659] 1 1.5165899999999999 False\n",
            "137 [0.372  4.5208] 0 1.5207999999999995 False\n",
            "138 [0.47    4.52501] 1 1.525009999999999 False\n",
            "139 [0.372   4.52922] 0 1.5292199999999987 False\n",
            "140 [0.47    4.53343] 1 1.5334299999999983 False\n",
            "141 [0.372   4.53764] 0 1.537639999999998 False\n",
            "142 [0.47    4.54185] 1 1.5418499999999975 False\n",
            "143 [0.372   4.54606] 0 1.546059999999997 False\n",
            "144 [0.274   4.54929] 0 1.5492899999999974 False\n",
            "145 [0.372   4.55252] 1 1.5525199999999977 False\n",
            "146 [0.274   4.55575] 0 1.555749999999998 False\n",
            "147 [0.372   4.55898] 1 1.5589799999999983 False\n",
            "148 [0.274   4.56221] 0 1.5622099999999985 False\n",
            "149 [0.372   4.56544] 1 1.5654399999999988 False\n",
            "150 [0.274   4.56867] 0 1.5686699999999991 False\n",
            "151 [0.372  4.5719] 1 1.5718999999999994 False\n",
            "152 [0.274   4.57513] 0 1.5751299999999997 False\n",
            "153 [0.372   4.57836] 1 1.57836 False\n",
            "154 [0.274   4.58159] 0 1.5815900000000003 False\n",
            "155 [0.372   4.58482] 1 1.5848200000000006 False\n",
            "156 [0.274   4.58805] 0 1.5880500000000008 False\n",
            "157 [0.372   4.59128] 1 1.5912800000000011 False\n",
            "158 [0.274   4.59451] 0 1.5945100000000014 False\n",
            "159 [0.176   4.59676] 0 1.5967600000000015 False\n",
            "160 [0.274   4.59901] 1 1.5990100000000016 False\n",
            "161 [0.176   4.60126] 0 1.6012600000000017 False\n",
            "162 [0.274   4.60351] 1 1.6035100000000018 False\n",
            "163 [0.176   4.60576] 0 1.6057600000000019 False\n",
            "164 [0.274   4.60801] 1 1.608010000000002 False\n",
            "165 [0.176   4.61026] 0 1.610260000000002 False\n",
            "166 [0.274   4.61251] 1 1.612510000000002 False\n",
            "167 [0.176   4.61476] 0 1.6147600000000022 False\n",
            "168 [0.274   4.61701] 1 1.6170100000000023 False\n",
            "169 [0.176   4.61926] 0 1.6192600000000024 False\n",
            "170 [0.274   4.62151] 1 1.6215100000000024 False\n",
            "171 [0.176   4.62376] 0 1.6237600000000025 False\n",
            "172 [0.274   4.62601] 1 1.6260100000000026 False\n",
            "173 [0.176   4.62826] 0 1.6282600000000027 False\n",
            "174 [0.274   4.63051] 1 1.6305100000000028 False\n",
            "175 [0.176   4.63276] 0 1.6327600000000029 False\n",
            "176 [0.274   4.63501] 1 1.635010000000003 False\n",
            "177 [0.176   4.63726] 0 1.637260000000003 False\n",
            "178 [0.274   4.63951] 1 1.6395100000000031 False\n",
            "179 [0.176   4.64176] 0 1.6417600000000032 False\n",
            "180 [0.274   4.64401] 1 1.6440100000000033 False\n",
            "181 [0.176   4.64626] 0 1.6462600000000034 False\n",
            "182 [0.274   4.64851] 1 1.6485100000000035 False\n",
            "183 [0.176   4.65076] 0 1.6507600000000036 False\n",
            "184 [0.274   4.65301] 1 1.6530100000000036 False\n",
            "185 [0.176   4.65526] 0 1.6552600000000037 False\n",
            "186 [0.078   4.65653] 0 1.6565300000000036 False\n",
            "187 [0.176  4.6578] 1 1.6578000000000035 False\n",
            "188 [0.078   4.65907] 0 1.6590700000000034 False\n",
            "189 [0.176   4.66034] 1 1.6603400000000033 False\n",
            "190 [0.078   4.66161] 0 1.6616100000000031 False\n",
            "191 [0.176   4.66288] 1 1.662880000000003 False\n",
            "192 [0.078   4.66415] 0 1.664150000000003 False\n",
            "193 [0.176   4.66542] 1 1.6654200000000028 False\n",
            "194 [0.078   4.66669] 0 1.6666900000000027 False\n",
            "195 [0.176   4.66796] 1 1.6679600000000026 False\n",
            "196 [0.078   4.66923] 0 1.6692300000000024 False\n",
            "197 [0.176  4.6705] 1 1.6705000000000023 False\n",
            "198 [0.078   4.67177] 0 1.6717700000000022 False\n",
            "199 [0.176   4.67304] 1 1.673040000000002 False\n",
            "200 [0.078   4.67431] 0 1.674310000000002 False\n",
            "201 [0.176   4.67558] 1 1.6755800000000018 False\n",
            "202 [0.078   4.67685] 0 1.6768500000000017 False\n",
            "203 [0.176   4.67812] 1 1.6781200000000016 False\n",
            "204 [0.078   4.67939] 0 1.6793900000000015 False\n",
            "205 [0.176   4.68066] 1 1.6806600000000014 False\n",
            "206 [0.078   4.68193] 0 1.6819300000000013 False\n",
            "207 [0.176  4.6832] 1 1.6832000000000011 False\n",
            "208 [0.078   4.68447] 0 1.684470000000001 False\n",
            "209 [0.176   4.68574] 1 1.685740000000001 False\n",
            "210 [0.078   4.68701] 0 1.6870100000000008 False\n",
            "211 [0.176   4.68828] 1 1.6882800000000007 False\n",
            "212 [0.078   4.68955] 0 1.6895500000000006 False\n",
            "213 [0.176   4.69082] 1 1.6908200000000004 False\n",
            "214 [0.078   4.69209] 0 1.6920900000000003 False\n",
            "215 [0.176   4.69336] 1 1.6933600000000002 False\n",
            "216 [0.078   4.69463] 0 1.69463 False\n",
            "217 [0.176  4.6959] 1 1.6959 False\n",
            "218 [0.078   4.69717] 0 1.6971699999999998 False\n",
            "219 [0.176   4.69844] 1 1.6984399999999997 False\n",
            "220 [0.078   4.69971] 0 1.6997099999999996 False\n",
            "221 [0.176   4.70098] 1 1.7009799999999995 False\n",
            "222 [0.078   4.70225] 0 1.7022499999999994 False\n",
            "223 [0.176   4.70352] 1 1.7035199999999993 False\n",
            "224 [0.078   4.70479] 0 1.7047899999999991 False\n",
            "225 [0.176   4.70606] 1 1.706059999999999 False\n",
            "226 [0.078   4.70733] 0 1.707329999999999 False\n",
            "227 [0.176  4.7086] 1 1.7085999999999988 False\n",
            "228 [0.078   4.70987] 0 1.7098699999999987 False\n",
            "229 [0.176   4.71114] 1 1.7111399999999986 False\n",
            "230 [0.078   4.71241] 0 1.7124099999999984 False\n",
            "231 [0.176   4.71368] 1 1.7136799999999983 False\n",
            "232 [0.078   4.71495] 0 1.7149499999999982 False\n",
            "233 [0.176   4.71622] 1 1.716219999999998 False\n",
            "234 [0.078   4.71749] 0 1.717489999999998 False\n",
            "235 [0.176   4.71876] 1 1.7187599999999978 False\n",
            "236 [0.078   4.72003] 0 1.7200299999999977 False\n",
            "237 [0.176  4.7213] 1 1.7212999999999976 False\n",
            "238 [0.078   4.72257] 0 1.7225699999999975 False\n",
            "239 [0.176   4.72384] 1 1.7238399999999974 False\n",
            "240 [0.078   4.72511] 0 1.7251099999999973 False\n",
            "241 [0.176   4.72638] 1 1.7263799999999971 False\n",
            "242 [0.078   4.72765] 0 1.727649999999997 False\n",
            "243 [0.176   4.72892] 1 1.728919999999997 False\n",
            "244 [0.078   4.73019] 0 1.7301899999999968 False\n",
            "245 [0.176   4.73146] 1 1.7314599999999967 False\n",
            "246 [0.078   4.73273] 0 1.7327299999999966 False\n",
            "247 [0.176 4.734] 1 1.7339999999999964 False\n",
            "248 [0.078   4.73527] 0 1.7352699999999963 False\n",
            "249 [0.176   4.73654] 1 1.7365399999999962 False\n",
            "250 [0.078   4.73781] 0 1.737809999999996 False\n",
            "251 [0.176   4.73908] 1 1.739079999999996 False\n",
            "252 [0.078   4.74035] 0 1.7403499999999958 False\n",
            "253 [-0.02     4.74064] 0 1.7406399999999955 False\n",
            "254 [0.078   4.74093] 1 1.7409299999999952 False\n",
            "255 [-0.02     4.74122] 0 1.7412199999999949 False\n",
            "256 [0.078   4.74151] 1 1.7415099999999946 False\n",
            "257 [-0.02    4.7418] 0 1.7417999999999942 False\n",
            "258 [0.078   4.74209] 1 1.742089999999994 False\n",
            "259 [-0.02     4.74238] 0 1.7423799999999936 False\n",
            "260 [0.078   4.74267] 1 1.7426699999999933 False\n",
            "261 [-0.02     4.74296] 0 1.742959999999993 False\n",
            "262 [0.078   4.74325] 1 1.7432499999999926 False\n",
            "263 [-0.02     4.74354] 0 1.7435399999999923 False\n",
            "264 [0.078   4.74383] 1 1.743829999999992 False\n",
            "265 [-0.02     4.74412] 0 1.7441199999999917 False\n",
            "266 [0.078   4.74441] 1 1.7444099999999914 False\n",
            "267 [-0.02    4.7447] 0 1.744699999999991 False\n",
            "268 [0.078   4.74499] 1 1.7449899999999907 False\n",
            "269 [-0.02     4.74528] 0 1.7452799999999904 False\n",
            "270 [0.078   4.74557] 1 1.74556999999999 False\n",
            "271 [-0.02     4.74586] 0 1.7458599999999898 False\n",
            "272 [0.078   4.74615] 1 1.7461499999999894 False\n",
            "273 [-0.02     4.74644] 0 1.7464399999999891 False\n",
            "274 [0.078   4.74673] 1 1.7467299999999888 False\n",
            "275 [-0.02     4.74702] 0 1.7470199999999885 False\n",
            "276 [0.078   4.74731] 1 1.7473099999999882 False\n",
            "277 [-0.02    4.7476] 0 1.7475999999999878 False\n",
            "278 [0.078   4.74789] 1 1.7478899999999875 False\n",
            "279 [-0.02     4.74818] 0 1.7481799999999872 False\n",
            "280 [0.078   4.74847] 1 1.7484699999999869 False\n",
            "281 [-0.02     4.74876] 0 1.7487599999999865 False\n",
            "282 [0.078   4.74905] 1 1.7490499999999862 False\n",
            "283 [-0.02     4.74934] 0 1.749339999999986 False\n",
            "284 [0.078   4.74963] 1 1.7496299999999856 False\n",
            "285 [-0.02     4.74992] 0 1.7499199999999853 False\n",
            "286 [0.078   4.75021] 1 1.750209999999985 False\n",
            "287 [-0.02    4.7505] 0 1.7504999999999846 False\n",
            "288 [0.078   4.75079] 1 1.7507899999999843 False\n",
            "289 [-0.02     4.75108] 0 1.751079999999984 False\n",
            "290 [0.078   4.75137] 1 1.7513699999999837 False\n",
            "291 [-0.02     4.75166] 0 1.7516599999999833 False\n",
            "292 [0.078   4.75195] 1 1.751949999999983 False\n",
            "293 [-0.02     4.75224] 0 1.7522399999999827 False\n",
            "294 [0.078   4.75253] 1 1.7525299999999824 False\n",
            "295 [-0.02     4.75282] 0 1.752819999999982 False\n",
            "296 [0.078   4.75311] 1 1.7531099999999817 False\n",
            "297 [-0.02    4.7534] 0 1.7533999999999814 False\n",
            "298 [0.078   4.75369] 1 1.753689999999981 False\n",
            "299 [-0.02     4.75398] 0 1.7539799999999808 False\n",
            "300 [0.078   4.75427] 1 1.7542699999999805 False\n",
            "301 [-0.02     4.75456] 0 1.7545599999999801 False\n",
            "302 [0.078   4.75485] 1 1.7548499999999798 False\n",
            "303 [-0.02     4.75514] 0 1.7551399999999795 False\n",
            "304 [-0.118    4.75445] 0 1.7544499999999799 False\n",
            "305 [-0.02     4.75376] 1 1.7537599999999802 False\n",
            "306 [0.078   4.75405] 1 1.75404999999998 False\n",
            "307 [-0.02     4.75434] 0 1.7543399999999796 False\n",
            "308 [0.078   4.75463] 1 1.7546299999999793 False\n",
            "309 [-0.02     4.75492] 0 1.754919999999979 False\n",
            "310 [0.078   4.75521] 1 1.7552099999999786 False\n",
            "311 [-0.02    4.7555] 0 1.7554999999999783 False\n",
            "312 [0.078   4.75579] 1 1.755789999999978 False\n",
            "313 [-0.02     4.75608] 0 1.7560799999999777 False\n",
            "314 [0.078   4.75637] 1 1.7563699999999773 False\n",
            "315 [-0.02     4.75666] 0 1.756659999999977 False\n",
            "316 [0.078   4.75695] 1 1.7569499999999767 False\n",
            "317 [-0.02     4.75724] 0 1.7572399999999764 False\n",
            "318 [0.078   4.75753] 1 1.757529999999976 False\n",
            "319 [-0.02     4.75782] 0 1.7578199999999757 False\n",
            "320 [0.078   4.75811] 1 1.7581099999999754 False\n",
            "321 [-0.02    4.7584] 0 1.758399999999975 False\n",
            "322 [0.078   4.75869] 1 1.7586899999999748 False\n",
            "323 [-0.02     4.75898] 0 1.7589799999999745 False\n",
            "324 [0.078   4.75927] 1 1.7592699999999741 False\n",
            "325 [-0.02     4.75956] 0 1.7595599999999738 False\n",
            "326 [0.078   4.75985] 1 1.7598499999999735 False\n",
            "327 [-0.02     4.76014] 0 1.7601399999999732 False\n",
            "328 [0.078   4.76043] 1 1.7604299999999728 False\n",
            "329 [-0.02     4.76072] 0 1.7607199999999725 False\n",
            "330 [-0.118    4.76003] 0 1.760029999999973 False\n",
            "331 [-0.02     4.75934] 1 1.7593399999999733 False\n",
            "332 [0.078   4.75963] 1 1.759629999999973 False\n",
            "333 [-0.02     4.75992] 0 1.7599199999999726 False\n",
            "334 [0.078   4.76021] 1 1.7602099999999723 False\n",
            "335 [-0.02    4.7605] 0 1.760499999999972 False\n",
            "336 [0.078   4.76079] 1 1.7607899999999717 False\n",
            "337 [-0.02     4.76108] 0 1.7610799999999713 False\n",
            "338 [0.078   4.76137] 1 1.761369999999971 False\n",
            "339 [-0.02     4.76166] 0 1.7616599999999707 False\n",
            "340 [0.078   4.76195] 1 1.7619499999999704 False\n",
            "341 [-0.02     4.76224] 0 1.76223999999997 False\n",
            "342 [0.078   4.76253] 1 1.7625299999999697 False\n",
            "343 [-0.02     4.76282] 0 1.7628199999999694 False\n",
            "344 [0.078   4.76311] 1 1.763109999999969 False\n",
            "345 [-0.02    4.7634] 0 1.7633999999999688 False\n",
            "346 [0.078   4.76369] 1 1.7636899999999684 False\n",
            "347 [-0.02     4.76398] 0 1.7639799999999681 False\n",
            "348 [0.078   4.76427] 1 1.7642699999999678 False\n",
            "349 [-0.02     4.76456] 0 1.7645599999999675 False\n",
            "350 [0.078   4.76485] 1 1.7648499999999672 False\n",
            "351 [-0.02     4.76514] 0 1.7651399999999668 False\n",
            "352 [0.078   4.76543] 1 1.7654299999999665 False\n",
            "353 [-0.02     4.76572] 0 1.7657199999999662 False\n",
            "354 [0.078   4.76601] 1 1.7660099999999659 False\n",
            "355 [-0.02    4.7663] 0 1.7662999999999656 False\n",
            "356 [0.078   4.76659] 1 1.7665899999999652 False\n",
            "357 [-0.02     4.76688] 0 1.766879999999965 False\n",
            "358 [0.078   4.76717] 1 1.7671699999999646 False\n",
            "359 [-0.02     4.76746] 0 1.7674599999999643 False\n",
            "360 [0.078   4.76775] 1 1.767749999999964 False\n",
            "361 [-0.02     4.76804] 0 1.7680399999999636 False\n",
            "362 [0.078   4.76833] 1 1.7683299999999633 False\n",
            "363 [-0.02     4.76862] 0 1.768619999999963 False\n",
            "364 [0.078   4.76891] 1 1.7689099999999627 False\n",
            "365 [-0.02    4.7692] 0 1.7691999999999624 False\n",
            "366 [0.078   4.76949] 1 1.769489999999962 False\n",
            "367 [-0.02     4.76978] 0 1.7697799999999617 False\n",
            "368 [0.078   4.77007] 1 1.7700699999999614 False\n",
            "369 [-0.02     4.77036] 0 1.770359999999961 False\n",
            "370 [0.078   4.77065] 1 1.7706499999999608 False\n",
            "371 [-0.02     4.77094] 0 1.7709399999999604 False\n",
            "372 [0.078   4.77123] 1 1.7712299999999601 False\n",
            "373 [-0.02     4.77152] 0 1.7715199999999598 False\n",
            "374 [0.078   4.77181] 1 1.7718099999999595 False\n",
            "375 [-0.02    4.7721] 0 1.7720999999999592 False\n",
            "376 [0.078   4.77239] 1 1.7723899999999588 False\n",
            "377 [-0.02     4.77268] 0 1.7726799999999585 False\n",
            "378 [0.078   4.77297] 1 1.7729699999999582 False\n",
            "379 [-0.02     4.77326] 0 1.7732599999999579 False\n",
            "380 [0.078   4.77355] 1 1.7735499999999575 False\n",
            "381 [-0.02     4.77384] 0 1.7738399999999572 False\n",
            "382 [0.078   4.77413] 1 1.774129999999957 False\n",
            "383 [-0.02     4.77442] 0 1.7744199999999566 False\n",
            "384 [0.078   4.77471] 1 1.7747099999999563 False\n",
            "385 [-0.02   4.775] 0 1.774999999999956 False\n",
            "386 [0.078   4.77529] 1 1.7752899999999556 False\n",
            "387 [-0.02     4.77558] 0 1.7755799999999553 False\n",
            "388 [0.078   4.77587] 1 1.775869999999955 False\n",
            "389 [-0.02     4.77616] 0 1.7761599999999547 False\n",
            "390 [0.078   4.77645] 1 1.7764499999999543 False\n",
            "391 [-0.02     4.77674] 0 1.776739999999954 False\n",
            "392 [0.078   4.77703] 1 1.7770299999999537 False\n",
            "393 [-0.02     4.77732] 0 1.7773199999999534 False\n",
            "394 [0.078   4.77761] 1 1.777609999999953 False\n",
            "395 [-0.02    4.7779] 0 1.7778999999999527 False\n",
            "396 [0.078   4.77819] 1 1.7781899999999524 False\n",
            "397 [-0.02     4.77848] 0 1.778479999999952 False\n",
            "398 [0.078   4.77877] 1 1.7787699999999518 False\n",
            "399 [-0.02     4.77906] 0 1.7790599999999515 False\n",
            "400 [0.078   4.77935] 1 1.7793499999999511 False\n",
            "401 [-0.02     4.77964] 0 1.7796399999999508 False\n",
            "402 [0.078   4.77993] 1 1.7799299999999505 False\n",
            "403 [-0.02     4.78022] 0 1.7802199999999502 False\n",
            "404 [0.078   4.78051] 1 1.7805099999999499 False\n",
            "405 [-0.02    4.7808] 0 1.7807999999999495 False\n",
            "406 [0.078   4.78109] 1 1.7810899999999492 False\n",
            "407 [-0.02     4.78138] 0 1.781379999999949 False\n",
            "408 [0.078   4.78167] 1 1.7816699999999486 False\n",
            "409 [-0.02     4.78196] 0 1.7819599999999483 False\n",
            "410 [0.078   4.78225] 1 1.782249999999948 False\n",
            "411 [-0.02     4.78254] 0 1.7825399999999476 False\n",
            "412 [0.078   4.78283] 1 1.7828299999999473 False\n",
            "413 [-0.02     4.78312] 0 1.783119999999947 False\n",
            "414 [0.078   4.78341] 1 1.7834099999999466 False\n",
            "415 [-0.02    4.7837] 0 1.7836999999999463 False\n",
            "416 [0.078   4.78399] 1 1.783989999999946 False\n",
            "417 [-0.02     4.78428] 0 1.7842799999999457 False\n",
            "418 [0.078   4.78457] 1 1.7845699999999454 False\n",
            "419 [-0.02     4.78486] 0 1.784859999999945 False\n",
            "420 [0.078   4.78515] 1 1.7851499999999447 False\n",
            "421 [-0.02     4.78544] 0 1.7854399999999444 False\n",
            "422 [0.078   4.78573] 1 1.785729999999944 False\n",
            "423 [-0.02     4.78602] 0 1.7860199999999438 False\n",
            "424 [0.078   4.78631] 1 1.7863099999999434 False\n",
            "425 [-0.02    4.7866] 0 1.7865999999999431 False\n",
            "426 [0.078   4.78689] 1 1.7868899999999428 False\n",
            "427 [-0.02     4.78718] 0 1.7871799999999425 False\n",
            "428 [0.078   4.78747] 1 1.7874699999999422 False\n",
            "429 [-0.02     4.78776] 0 1.7877599999999418 False\n",
            "430 [0.078   4.78805] 1 1.7880499999999415 False\n",
            "431 [-0.02     4.78834] 0 1.7883399999999412 False\n",
            "432 [0.078   4.78863] 1 1.7886299999999409 False\n",
            "433 [-0.02     4.78892] 0 1.7889199999999406 False\n",
            "434 [0.078   4.78921] 1 1.7892099999999402 False\n",
            "435 [-0.02    4.7895] 0 1.78949999999994 False\n",
            "436 [0.078   4.78979] 1 1.7897899999999396 False\n",
            "437 [-0.02     4.79008] 0 1.7900799999999393 False\n",
            "438 [0.078   4.79037] 1 1.790369999999939 False\n",
            "439 [-0.02     4.79066] 0 1.7906599999999386 False\n",
            "440 [0.078   4.79095] 1 1.7909499999999383 False\n",
            "441 [-0.02     4.79124] 0 1.791239999999938 False\n",
            "442 [0.078   4.79153] 1 1.7915299999999377 False\n",
            "443 [-0.02     4.79182] 0 1.7918199999999374 False\n",
            "444 [0.078   4.79211] 1 1.792109999999937 False\n",
            "445 [-0.02    4.7924] 0 1.7923999999999367 False\n",
            "446 [0.078   4.79269] 1 1.7926899999999364 False\n",
            "447 [-0.02     4.79298] 0 1.792979999999936 False\n",
            "448 [0.078   4.79327] 1 1.7932699999999357 False\n",
            "449 [-0.02     4.79356] 0 1.7935599999999354 False\n",
            "450 [0.078   4.79385] 1 1.793849999999935 False\n",
            "451 [-0.02     4.79414] 0 1.7941399999999348 False\n",
            "452 [0.078   4.79443] 1 1.7944299999999345 False\n",
            "453 [-0.02     4.79472] 0 1.7947199999999341 False\n",
            "454 [0.078   4.79501] 1 1.7950099999999338 False\n",
            "455 [-0.02    4.7953] 0 1.7952999999999335 False\n",
            "456 [0.078   4.79559] 1 1.7955899999999332 False\n",
            "457 [-0.02     4.79588] 0 1.7958799999999329 False\n",
            "458 [0.078   4.79617] 1 1.7961699999999325 False\n",
            "459 [-0.02     4.79646] 0 1.7964599999999322 False\n",
            "460 [0.078   4.79675] 1 1.796749999999932 False\n",
            "461 [-0.02     4.79704] 0 1.7970399999999316 False\n",
            "462 [0.078   4.79733] 1 1.7973299999999313 False\n",
            "463 [-0.02     4.79762] 0 1.797619999999931 False\n",
            "464 [0.078   4.79791] 1 1.7979099999999306 False\n",
            "465 [-0.02    4.7982] 0 1.7981999999999303 False\n",
            "466 [0.078   4.79849] 1 1.79848999999993 False\n",
            "467 [-0.02     4.79878] 0 1.7987799999999297 False\n",
            "468 [0.078   4.79907] 1 1.7990699999999293 False\n",
            "469 [-0.02     4.79936] 0 1.799359999999929 False\n",
            "470 [0.078   4.79965] 1 1.7996499999999287 False\n",
            "471 [-0.02     4.79994] 0 1.7999399999999284 False\n",
            "472 [0.078   4.80023] 1 1.800229999999928 False\n",
            "473 [-0.02     4.80052] 0 1.8005199999999277 False\n",
            "474 [0.078   4.80081] 1 1.8008099999999274 False\n",
            "475 [-0.02    4.8011] 0 1.801099999999927 False\n",
            "476 [0.078   4.80139] 1 1.8013899999999268 False\n",
            "477 [-0.02     4.80168] 0 1.8016799999999265 False\n",
            "478 [0.078   4.80197] 1 1.8019699999999261 False\n",
            "479 [-0.02     4.80226] 0 1.8022599999999258 False\n",
            "480 [0.078   4.80255] 1 1.8025499999999255 False\n",
            "481 [-0.02     4.80284] 0 1.8028399999999252 False\n",
            "482 [0.078   4.80313] 1 1.8031299999999248 False\n",
            "483 [-0.02     4.80342] 0 1.8034199999999245 False\n",
            "484 [-0.118    4.80273] 0 1.802729999999925 False\n",
            "485 [-0.02     4.80204] 1 1.8020399999999253 False\n",
            "486 [0.078   4.80233] 1 1.802329999999925 False\n",
            "487 [-0.02     4.80262] 0 1.8026199999999246 False\n",
            "488 [0.078   4.80291] 1 1.8029099999999243 False\n",
            "489 [-0.02    4.8032] 0 1.803199999999924 False\n",
            "490 [0.078   4.80349] 1 1.8034899999999237 False\n",
            "491 [-0.02     4.80378] 0 1.8037799999999233 False\n",
            "492 [0.078   4.80407] 1 1.804069999999923 False\n",
            "493 [-0.02     4.80436] 0 1.8043599999999227 False\n",
            "494 [0.078   4.80465] 1 1.8046499999999224 False\n",
            "495 [-0.02     4.80494] 0 1.804939999999922 False\n",
            "496 [0.078   4.80523] 1 1.8052299999999217 False\n",
            "497 [-0.02     4.80552] 0 1.8055199999999214 False\n",
            "498 [0.078   4.80581] 1 1.805809999999921 False\n",
            "499 [-0.02    4.8061] 0 1.8060999999999208 False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nyntTKdqiWP",
        "colab_type": "text"
      },
      "source": [
        "We can now plot the position vs. the steps for the test case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z6ctyygqiWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "239e59e1-495d-41d1-ff60-df484c0e0c9c"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.set_ylim([0,10])\n",
        "plt.plot(S[:,1]); plt.xlabel('steps'); plt.ylabel('position')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'position')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEKCAYAAAAVaT4rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTBWhP6cqiWV",
        "colab_type": "text"
      },
      "source": [
        "We can also plot the policy (the action being taken) in different regions of the state space and overlay the movement of the ball in the test case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGySd7q4qiWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "3d5b4674-a50f-4424-9eaf-975c0cf1bf54"
      },
      "source": [
        "#%% Plotting the policy in the state space.\n",
        "x = np.linspace(0, 10, 50)\n",
        "v = np.linspace(-20, 20, 60)\n",
        "A = np.zeros((len(x),len(v)))\n",
        "for i,xi in enumerate(x):\n",
        "    for j,vj in enumerate(v):\n",
        "        A[i,j] = select_action(FloatTensor([[vj,xi]])).data.numpy()[0,0]\n",
        "plt.figure(3)\n",
        "plt.contourf(v,x,A,levels=[0.1,1]);\n",
        "plt.xlabel(\"v\"); plt.ylabel(\"x\")\n",
        "plt.scatter(S[:,0],S[:,1],c='r'); plt.plot(S[0,0],S[0,1],c = 'k', marker ='*'); plt.scatter(S[-1,0],S[-1,1],c = 'k', marker ='s')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f4cd1727550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsC1CqYlqiWX",
        "colab_type": "text"
      },
      "source": [
        "## Further Exercises\n",
        "\n",
        "1. What is the response of the neural network if the mass of the ball is changed in testing?\n",
        "2. Write a reinforcement learning program which learns a policy for every possible mass of the ball from 0.2 to 1.9 kgs.\n",
        "3. Write a reinforcement learning program which learns a policy for every possible integer reference point between 0 and 10.\n",
        "\n",
        "### Acknowledgements\n",
        "I would like to recognize the effort by Noushan and Abdullah for their contributions in the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgHPrGLiqiWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}